torch>=2.0.0
transformers>=4.36.0
datasets>=2.15.0
accelerate>=0.25.0
tqdm>=4.66.0
pandas>=2.0.0
numpy>=1.24.0
sentencepiece>=0.1.99
wandb>=0.16.0
jsonlines>=4.0.0
einops>=0.7.0
bitsandbytes>=0.41.0  # For 8-bit training
deepspeed>=0.12.0  # For distributed training
peft>=0.7.0  # For parameter efficient fine-tuning
evaluate>=0.4.0
tokenizers>=0.15.0  # For tokenizer training
tiktoken>=0.5.0  # For BPE tokenization
protobuf>=4.24.4  # Required by some tokenizer operations 