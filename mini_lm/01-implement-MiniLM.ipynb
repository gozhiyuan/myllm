{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22202,
     "status": "ok",
     "timestamp": 1743922117845,
     "user": {
      "displayName": "Leon L",
      "userId": "18379634264938533801"
     },
     "user_tz": 420
    },
    "id": "omvSL36txTh5",
    "outputId": "d9741f35-adb3-4245-9da3-a134c285fb04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17796,
     "status": "ok",
     "timestamp": 1743922867003,
     "user": {
      "displayName": "Leon L",
      "userId": "18379634264938533801"
     },
     "user_tz": 420
    },
    "id": "6D37FTBrDmmt",
    "outputId": "647c1fb4-59ac-4c99-f228-a273f9e9e33d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(49152, 576, padding_idx=2)\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
      ")\n",
      "<|im_start|>user\n",
      "What is gravity?<|im_end|>\n",
      "\n",
      "<|im_start|>user\n",
      "What is gravity?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Gravity is a fundamental force of nature that governs the motion of objects in the universe. It is a universal force that attracts two objects with mass towards each other, regardless of their distance or speed. Gravity is a consequence of the\n",
      "model.embed_tokens.weight torch.Size([49152, 576])\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([576, 576])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([192, 576])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([192, 576])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([576, 576])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([1536, 576])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([1536, 576])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([576, 1536])\n",
      "model.layers.0.input_layernorm.weight torch.Size([576])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([576])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([576, 576])\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([192, 576])\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([192, 576])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([576, 576])\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([1536, 576])\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([1536, 576])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([576, 1536])\n",
      "model.layers.1.input_layernorm.weight torch.Size([576])\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([576])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([576, 576])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([192, 576])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([192, 576])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([576, 576])\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([1536, 576])\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([1536, 576])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([576, 1536])\n",
      "model.layers.2.input_layernorm.weight torch.Size([576])\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([576])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([576, 576])\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([192, 576])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([192, 576])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([576, 576])\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([1536, 576])\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([1536, 576])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([576, 1536])\n",
      "model.layers.3.input_layernorm.weight torch.Size([576])\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([576])\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([576, 576])\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([192, 576])\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([192, 576])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([576, 576])\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([1536, 576])\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([1536, 576])\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([576, 1536])\n",
      "model.layers.4.input_layernorm.weight torch.Size([576])\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([576])\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([576, 576])\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([192, 576])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([192, 576])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([576, 576])\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([1536, 576])\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([1536, 576])\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([576, 1536])\n",
      "model.layers.5.input_layernorm.weight torch.Size([576])\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([576])\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([576, 576])\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([192, 576])\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([192, 576])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([576, 576])\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([1536, 576])\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([1536, 576])\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([576, 1536])\n",
      "model.layers.6.input_layernorm.weight torch.Size([576])\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([576])\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([576, 576])\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([192, 576])\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([192, 576])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([576, 576])\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([1536, 576])\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([1536, 576])\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([576, 1536])\n",
      "model.layers.7.input_layernorm.weight torch.Size([576])\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([576])\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([576, 576])\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([192, 576])\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([192, 576])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([576, 576])\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([1536, 576])\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([1536, 576])\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([576, 1536])\n",
      "model.layers.8.input_layernorm.weight torch.Size([576])\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([576])\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([576, 576])\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([192, 576])\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([192, 576])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([576, 576])\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([1536, 576])\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([1536, 576])\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([576, 1536])\n",
      "model.layers.9.input_layernorm.weight torch.Size([576])\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([576])\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([576, 576])\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([192, 576])\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([192, 576])\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([576, 576])\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([1536, 576])\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([1536, 576])\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([576, 1536])\n",
      "model.layers.10.input_layernorm.weight torch.Size([576])\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([576])\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([576, 576])\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([192, 576])\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([192, 576])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([576, 576])\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([1536, 576])\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([1536, 576])\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([576, 1536])\n",
      "model.layers.11.input_layernorm.weight torch.Size([576])\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([576])\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([576, 576])\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([192, 576])\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([192, 576])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([576, 576])\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([1536, 576])\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([1536, 576])\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([576, 1536])\n",
      "model.layers.12.input_layernorm.weight torch.Size([576])\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([576])\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([576, 576])\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([192, 576])\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([192, 576])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([576, 576])\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([1536, 576])\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([1536, 576])\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([576, 1536])\n",
      "model.layers.13.input_layernorm.weight torch.Size([576])\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([576])\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([576, 576])\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([192, 576])\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([192, 576])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([576, 576])\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([1536, 576])\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([1536, 576])\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([576, 1536])\n",
      "model.layers.14.input_layernorm.weight torch.Size([576])\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([576])\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([576, 576])\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([192, 576])\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([192, 576])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([576, 576])\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([1536, 576])\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([1536, 576])\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([576, 1536])\n",
      "model.layers.15.input_layernorm.weight torch.Size([576])\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([576])\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([576, 576])\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([192, 576])\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([192, 576])\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([576, 576])\n",
      "model.layers.16.mlp.gate_proj.weight torch.Size([1536, 576])\n",
      "model.layers.16.mlp.up_proj.weight torch.Size([1536, 576])\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([576, 1536])\n",
      "model.layers.16.input_layernorm.weight torch.Size([576])\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([576])\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([576, 576])\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([192, 576])\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([192, 576])\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([576, 576])\n",
      "model.layers.17.mlp.gate_proj.weight torch.Size([1536, 576])\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([1536, 576])\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([576, 1536])\n",
      "model.layers.17.input_layernorm.weight torch.Size([576])\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([576])\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([576, 576])\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([192, 576])\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([192, 576])\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([576, 576])\n",
      "model.layers.18.mlp.gate_proj.weight torch.Size([1536, 576])\n",
      "model.layers.18.mlp.up_proj.weight torch.Size([1536, 576])\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([576, 1536])\n",
      "model.layers.18.input_layernorm.weight torch.Size([576])\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([576])\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([576, 576])\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([192, 576])\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([192, 576])\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([576, 576])\n",
      "model.layers.19.mlp.gate_proj.weight torch.Size([1536, 576])\n",
      "model.layers.19.mlp.up_proj.weight torch.Size([1536, 576])\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([576, 1536])\n",
      "model.layers.19.input_layernorm.weight torch.Size([576])\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([576])\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([576, 576])\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([192, 576])\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([192, 576])\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([576, 576])\n",
      "model.layers.20.mlp.gate_proj.weight torch.Size([1536, 576])\n",
      "model.layers.20.mlp.up_proj.weight torch.Size([1536, 576])\n",
      "model.layers.20.mlp.down_proj.weight torch.Size([576, 1536])\n",
      "model.layers.20.input_layernorm.weight torch.Size([576])\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([576])\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([576, 576])\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([192, 576])\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([192, 576])\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([576, 576])\n",
      "model.layers.21.mlp.gate_proj.weight torch.Size([1536, 576])\n",
      "model.layers.21.mlp.up_proj.weight torch.Size([1536, 576])\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([576, 1536])\n",
      "model.layers.21.input_layernorm.weight torch.Size([576])\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([576])\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([576, 576])\n",
      "model.layers.22.self_attn.k_proj.weight torch.Size([192, 576])\n",
      "model.layers.22.self_attn.v_proj.weight torch.Size([192, 576])\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([576, 576])\n",
      "model.layers.22.mlp.gate_proj.weight torch.Size([1536, 576])\n",
      "model.layers.22.mlp.up_proj.weight torch.Size([1536, 576])\n",
      "model.layers.22.mlp.down_proj.weight torch.Size([576, 1536])\n",
      "model.layers.22.input_layernorm.weight torch.Size([576])\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([576])\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([576, 576])\n",
      "model.layers.23.self_attn.k_proj.weight torch.Size([192, 576])\n",
      "model.layers.23.self_attn.v_proj.weight torch.Size([192, 576])\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([576, 576])\n",
      "model.layers.23.mlp.gate_proj.weight torch.Size([1536, 576])\n",
      "model.layers.23.mlp.up_proj.weight torch.Size([1536, 576])\n",
      "model.layers.23.mlp.down_proj.weight torch.Size([576, 1536])\n",
      "model.layers.23.input_layernorm.weight torch.Size([576])\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([576])\n",
      "model.layers.24.self_attn.q_proj.weight torch.Size([576, 576])\n",
      "model.layers.24.self_attn.k_proj.weight torch.Size([192, 576])\n",
      "model.layers.24.self_attn.v_proj.weight torch.Size([192, 576])\n",
      "model.layers.24.self_attn.o_proj.weight torch.Size([576, 576])\n",
      "model.layers.24.mlp.gate_proj.weight torch.Size([1536, 576])\n",
      "model.layers.24.mlp.up_proj.weight torch.Size([1536, 576])\n",
      "model.layers.24.mlp.down_proj.weight torch.Size([576, 1536])\n",
      "model.layers.24.input_layernorm.weight torch.Size([576])\n",
      "model.layers.24.post_attention_layernorm.weight torch.Size([576])\n",
      "model.layers.25.self_attn.q_proj.weight torch.Size([576, 576])\n",
      "model.layers.25.self_attn.k_proj.weight torch.Size([192, 576])\n",
      "model.layers.25.self_attn.v_proj.weight torch.Size([192, 576])\n",
      "model.layers.25.self_attn.o_proj.weight torch.Size([576, 576])\n",
      "model.layers.25.mlp.gate_proj.weight torch.Size([1536, 576])\n",
      "model.layers.25.mlp.up_proj.weight torch.Size([1536, 576])\n",
      "model.layers.25.mlp.down_proj.weight torch.Size([576, 1536])\n",
      "model.layers.25.input_layernorm.weight torch.Size([576])\n",
      "model.layers.25.post_attention_layernorm.weight torch.Size([576])\n",
      "model.layers.26.self_attn.q_proj.weight torch.Size([576, 576])\n",
      "model.layers.26.self_attn.k_proj.weight torch.Size([192, 576])\n",
      "model.layers.26.self_attn.v_proj.weight torch.Size([192, 576])\n",
      "model.layers.26.self_attn.o_proj.weight torch.Size([576, 576])\n",
      "model.layers.26.mlp.gate_proj.weight torch.Size([1536, 576])\n",
      "model.layers.26.mlp.up_proj.weight torch.Size([1536, 576])\n",
      "model.layers.26.mlp.down_proj.weight torch.Size([576, 1536])\n",
      "model.layers.26.input_layernorm.weight torch.Size([576])\n",
      "model.layers.26.post_attention_layernorm.weight torch.Size([576])\n",
      "model.layers.27.self_attn.q_proj.weight torch.Size([576, 576])\n",
      "model.layers.27.self_attn.k_proj.weight torch.Size([192, 576])\n",
      "model.layers.27.self_attn.v_proj.weight torch.Size([192, 576])\n",
      "model.layers.27.self_attn.o_proj.weight torch.Size([576, 576])\n",
      "model.layers.27.mlp.gate_proj.weight torch.Size([1536, 576])\n",
      "model.layers.27.mlp.up_proj.weight torch.Size([1536, 576])\n",
      "model.layers.27.mlp.down_proj.weight torch.Size([576, 1536])\n",
      "model.layers.27.input_layernorm.weight torch.Size([576])\n",
      "model.layers.27.post_attention_layernorm.weight torch.Size([576])\n",
      "model.layers.28.self_attn.q_proj.weight torch.Size([576, 576])\n",
      "model.layers.28.self_attn.k_proj.weight torch.Size([192, 576])\n",
      "model.layers.28.self_attn.v_proj.weight torch.Size([192, 576])\n",
      "model.layers.28.self_attn.o_proj.weight torch.Size([576, 576])\n",
      "model.layers.28.mlp.gate_proj.weight torch.Size([1536, 576])\n",
      "model.layers.28.mlp.up_proj.weight torch.Size([1536, 576])\n",
      "model.layers.28.mlp.down_proj.weight torch.Size([576, 1536])\n",
      "model.layers.28.input_layernorm.weight torch.Size([576])\n",
      "model.layers.28.post_attention_layernorm.weight torch.Size([576])\n",
      "model.layers.29.self_attn.q_proj.weight torch.Size([576, 576])\n",
      "model.layers.29.self_attn.k_proj.weight torch.Size([192, 576])\n",
      "model.layers.29.self_attn.v_proj.weight torch.Size([192, 576])\n",
      "model.layers.29.self_attn.o_proj.weight torch.Size([576, 576])\n",
      "model.layers.29.mlp.gate_proj.weight torch.Size([1536, 576])\n",
      "model.layers.29.mlp.up_proj.weight torch.Size([1536, 576])\n",
      "model.layers.29.mlp.down_proj.weight torch.Size([576, 1536])\n",
      "model.layers.29.input_layernorm.weight torch.Size([576])\n",
      "model.layers.29.post_attention_layernorm.weight torch.Size([576])\n",
      "model.norm.weight torch.Size([576])\n",
      "lm_head.weight torch.Size([49152, 576])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "checkpoint = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if hasattr(torch, 'mps') and torch.mps.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "print(model)\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is gravity?\"}]\n",
    "input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(input_text)\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=50, temperature=0.2, top_p=0.9, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "for k,v in model.state_dict().items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1743922821869,
     "user": {
      "displayName": "Leon L",
      "userId": "18379634264938533801"
     },
     "user_tz": 420
    },
    "id": "R7_qyyucS_x4",
    "outputId": "712efbec-4f2f-40c4-a411-bd78d9b5e37c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special Tokens Map:\n",
      "{'bos_token': '<|im_start|>', 'eos_token': '<|im_end|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|im_end|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}\n",
      "\n",
      "Model Configuration:\n",
      "LlamaConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 576,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 9,\n",
      "  \"num_hidden_layers\": 30,\n",
      "  \"num_key_value_heads\": 3,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49152\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Print tokenizer configuration\n",
    "# print(\"Tokenizer Configuration:\")\n",
    "# print(tokenizer.config)\n",
    "print(\"Special Tokens Map:\")\n",
    "print(tokenizer.special_tokens_map)\n",
    "\n",
    "# Print model configuration\n",
    "print(\"\\nModel Configuration:\")\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhTS_9oyXTYe"
   },
   "source": [
    "# Build MiniLM Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkkAb08j_byj"
   },
   "source": [
    "## Build Model Using LlamaConfig\n",
    "As there is no official smolLM model architecture in huggingface yet, we can build it based on huggingface Llama implementations. Huggingface implementation is very modulized and we can easily make our own customized modules.\n",
    "\n",
    "Below example shows how to customize one transformer layer by modifying the MLP component, replacing the default SwiGLU (gated SiLU activation) with a simpler ReLU-based MLP. This demonstrates how to alter Hugging Face models using nn.Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2085,
     "status": "ok",
     "timestamp": 1743372551088,
     "user": {
      "displayName": "Leon L",
      "userId": "18379634264938533801"
     },
     "user_tz": 420
    },
    "id": "cPWuU9ZVXY8y",
    "outputId": "8936f64b-ac9e-4cf1-ee93-9f17c91b4b19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 576,\n",
      "  \"initializer_range\": 0.041666666666666664,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"is_llama_config\": true,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 9,\n",
      "  \"num_hidden_layers\": 30,\n",
      "  \"num_key_value_heads\": 3,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_interleaved\": false,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers.js_config\": {\n",
      "    \"kv_cache_dtype\": {\n",
      "      \"fp16\": \"float16\",\n",
      "      \"q4f16\": \"float16\"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49152\n",
      "}\n",
      "\n",
      "LlamaConfig {\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 576,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 9,\n",
      "  \"num_hidden_layers\": 30,\n",
      "  \"num_key_value_heads\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 20000\n",
      "}\n",
      "\n",
      "MiniLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(20000, 576)\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-06)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((576,), eps=1e-06)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=576, out_features=20000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import LlamaConfig, AutoConfig\n",
    "from transformers.models.llama.modeling_llama import LlamaDecoderLayer\n",
    "from transformers import LlamaForCausalLM\n",
    "\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "print(config)\n",
    "\n",
    "# Updated configuration\n",
    "config = LlamaConfig(\n",
    "    vocab_size=20000,\n",
    "    hidden_size=576,\n",
    "    intermediate_size=1536,\n",
    "    num_hidden_layers=30,\n",
    "    num_attention_heads=9,\n",
    "    num_key_value_heads=3,\n",
    "    max_position_embeddings=2048\n",
    ")\n",
    "print(config)\n",
    "\n",
    "# Custom MLP (unchanged)\n",
    "class CustomMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.up_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
    "        self.act_fn = F.relu  # ReLU instead of SwiGLU\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up_proj(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.down_proj(x)\n",
    "        return x\n",
    "\n",
    "# Custom Decoder Layer (unchanged)\n",
    "class CustomDecoderLayer(LlamaDecoderLayer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.mlp = CustomMLP(config)  # Replace default MLP\n",
    "\n",
    "# MiniLM with one custom layer\n",
    "class MiniLM(LlamaForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # self.model.layers[9] = CustomDecoderLayer(config)  # 10th layer (index 9)\n",
    "\n",
    "# Instantiate the model\n",
    "model = MiniLM(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KGvB-O86usXj"
   },
   "outputs": [],
   "source": [
    "# You can also convert to MOE layer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers.models.llama.modeling_llama import LlamaDecoderLayer\n",
    "\n",
    "\n",
    "class MoELayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_experts, expert_size):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        # Define a list of expert networks\n",
    "        self.experts = nn.ModuleList([nn.Linear(hidden_size, expert_size) for _ in range(num_experts)])\n",
    "        # Gating network to assign weights to experts\n",
    "        self.gate = nn.Linear(hidden_size, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute gating scores (softmax over expert dimension)\n",
    "        gate_scores = F.softmax(self.gate(x), dim=-1)  # Shape: [batch_size, num_experts]\n",
    "\n",
    "        # Compute outputs from all experts\n",
    "        expert_outputs = [expert(x) for expert in self.experts]  # List of [batch_size, expert_size]\n",
    "        expert_outputs = torch.stack(expert_outputs, dim=1)  # Shape: [batch_size, num_experts, expert_size]\n",
    "\n",
    "        # Weighted combination of expert outputs\n",
    "        output = torch.einsum('bse,bs->be', expert_outputs, gate_scores)  # Shape: [batch_size, expert_size]\n",
    "        return output\n",
    "\n",
    "\n",
    "class CustomMoEDecoderLayer(LlamaDecoderLayer):\n",
    "    def __init__(self, config, num_experts=4, expert_size=2048):\n",
    "        super().__init__(config)\n",
    "        # Replace the MLP (FFN) with MoE\n",
    "        self.mlp = MoELayer(config.hidden_size, num_experts, expert_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hwv3ULbnBLkL"
   },
   "source": [
    "## Build Model from Scratch\n",
    "Build based on current LlamaConfig implementation is not challenge to you? Let's build from scratch! There are already many very good resources building small llm from scratch, for example [nanogpt](https://github.com/karpathy/build-nanogpt/tree/master?tab=readme-ov-file), [tinyllama](https://github.com/jzhang38/TinyLlama/tree/main), [minimind](https://github.com/jingyaogong/minimind/tree/master) and more. Here we will step on their shoulders and build our own miniLM based on modified smolLM structure. We will still build on huggingface, which allow us to leverage trainer to train our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0xTG3w3mZjp"
   },
   "source": [
    "### Pull the smolLM weights\n",
    "We first pull the smolLM weights, and we will copy the weights to our miniLM implementations to make sure the model structure is aligned. See details in the `smol_model_copy_wrights.py`. We will use `smol_model.py` with the same model architecture but without copy the weights for model experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13043,
     "status": "ok",
     "timestamp": 1743922961706,
     "user": {
      "displayName": "Leon L",
      "userId": "18379634264938533801"
     },
     "user_tz": 420
    },
    "id": "KWu4R2xgFxTk",
    "outputId": "62e78c06-2439-4728-8cae-9d214e6a2acf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu124\n",
      "Python Version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
      "--- Loading HuggingFaceTB/SmolLM-135M-Instruct and preparing custom model for comparison ---\n",
      "--- Starting Comparison Setup for HuggingFaceTB/SmolLM-135M-Instruct ---\n",
      "Loading tokenizer from HuggingFaceTB/SmolLM-135M-Instruct\n",
      "Target device: cpu\n",
      "\n",
      "STEP 1: Loading official model with accelerate optimizations...\n",
      "STEP 1 SUCCESS: Official model loaded.\n",
      "  Official Model Device Map: {'': 'cpu'}\n",
      "\n",
      "STEP 2: Initializing custom SmolLM model on CPU...\n",
      "Initialized SmolLMConfig: GQA with 3 groups.\n",
      "Tied input and output embedding weights.\n",
      "STEP 2 SUCCESS: Custom model initialized successfully on CPU.\n",
      "\n",
      "STEP 3: Transferring weights to custom model (on CPU)...\n",
      "  Copying embed_tokens...\n",
      "  Copying layer 29/29...\n",
      "  Layer copies finished.\n",
      "  Copying final norm...\n",
      "  Ensuring weights are tied post-copy...\n",
      "Tied input and output embedding weights.\n",
      "STEP 3 SUCCESS: Weight transfer complete (on CPU).\n",
      "\n",
      "STEP 4: Moving custom model from CPU to target device: cpu...\n",
      "STEP 4 SUCCESS: Custom model moved to target device (cpu).\n",
      "\n",
      "--- Comparison Setup Complete ---\n",
      "\n",
      "==================== TESTING OFFICIAL MODEL ====================\n",
      "\n",
      "--- Testing Official Model ---\n",
      "Using device: cpu\n",
      "Model parameters are on: cpu\n",
      "Input shape: torch.Size([1, 9])\n",
      "Generating response...\n",
      "\n",
      "Prompt: Question: What is gravity?\n",
      "Answer:\n",
      "Response from Official Model:\n",
      "Question: What is gravity?\n",
      "Answer: Gravity is a force that attracts two objects with mass towards each other. It is a universal force that is responsible for the acceleration of objects.\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "============================================================\n",
      "\n",
      "==================== TESTING CUSTOM MODEL ====================\n",
      "\n",
      "--- Testing Custom Model ---\n",
      "Using device: cpu\n",
      "Model parameters are on: cpu\n",
      "Input shape: torch.Size([1, 9])\n",
      "Generating response...\n",
      "\n",
      "Prompt: Question: What is gravity?\n",
      "Answer:\n",
      "Response from Custom Model:\n",
      "Question: What is gravity?\n",
      "Answer: Gravity is a force that attracts two objects with mass towards each other, causing them to fall towards the center of the Earth.\n",
      "\n",
      "**3. What is the concept of gravity?**\n",
      "Answer: Gravity\n",
      "----------------------------------------\n",
      "============================================================\n",
      "\n",
      "Performing final cleanup...\n",
      "  Deleting official model reference...\n",
      "  Deleting custom model reference...\n",
      "  Deleting tokenizer reference...\n",
      "  Running garbage collection and clearing cache...\n",
      "Cleanup finished.\n"
     ]
    }
   ],
   "source": [
    "from smol_model import initialize_model, run_test\n",
    "\n",
    "model, tokenizer, device = initialize_model()\n",
    "\n",
    "run_test(model, \"SmolLM-135M-Instruct\", tokenizer, device, \"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8199,
     "status": "ok",
     "timestamp": 1743381005842,
     "user": {
      "displayName": "Leon L",
      "userId": "18379634264938533801"
     },
     "user_tz": 420
    },
    "id": "MsTPcH75Xg-6",
    "outputId": "440252d0-a6a8-442f-d7d6-4373afa7d815"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (0.5.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "executionInfo": {
     "elapsed": 15912,
     "status": "ok",
     "timestamp": 1743381695099,
     "user": {
      "displayName": "Leon L",
      "userId": "18379634264938533801"
     },
     "user_tz": 420
    },
    "id": "aVHuHd8AQ5AD",
    "outputId": "9921fe9e-47af-4225-e068-0b0c79efb440"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:12, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.040290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=3.022372007369995, metrics={'train_runtime': 14.0757, 'train_samples_per_second': 0.071, 'train_steps_per_second': 0.071, 'total_flos': 0.0, 'train_loss': 3.022372007369995, 'epoch': 1.0})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"tiny_shakespeare\")  # Example dataset\n",
    "tokenized_dataset = dataset.map(lambda x: tokenizer(x[\"text\"], truncation=True, max_length=2048), batched=True)\n",
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "eval_dataset = tokenized_dataset[\"test\"]\n",
    "\n",
    "# for i in train_dataset:\n",
    "#   print(i.keys(), len(i[\"input_ids\"]))\n",
    "#   break\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./minillm_output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=500,\n",
    "    save_safetensors=False,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# # Save the trained model\n",
    "# model.save_pretrained(\"./minillm_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ApH4uOogQ5Dr"
   },
   "outputs": [],
   "source": [
    "# Next: better init.\n",
    "# mixed precision training\n",
    "# torch.compile\n",
    "# flash attention. Done\n",
    "# change ugly numbers\n",
    "\n",
    "# Optimization:\n",
    "# adam init\n",
    "# norm clipping\n",
    "# cosine decay learning rate with warm up\n",
    "# weight decay. not include i-d tensors. fused adam\n",
    "# bs = 0.5M through gradient accumulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1p349wmNBOh9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zw_mwbQGbC9g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "49CS29ZQjwFT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F7naK8dkjwIF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOrr1B0BfVIdB1KlTCYKNHl",
   "collapsed_sections": [
    "KkkAb08j_byj"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
