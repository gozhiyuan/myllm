{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNoqhql87g/3c//Gk27YjuZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"kK7SmumcAZJc"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# Model Pretrain\n","\n","The script uses the tiny Shakespeare dataset for pretraining and configures the Hugging Face Trainer for efficient and optimized training.\n","\n","We use follow optimizations for the training:\n","- Weight initialization from the original SmolLM\n","- Mixed precision training (fp16)\n","- torch.compile for acceleration\n","- Optimizer with the requested configuration:\n","  - Adam with proper initialization\n","  - Gradient norm clipping (1.0)\n","  - Cosine decay learning rate with warmup\n","  - Weight decay (0.1) excluding LayerNorm and bias parameters\n","  - Fused Adam implementation when available\n","  - Large effective batch size through gradient accumulation\n","\n","\n","The training data setup in this script is based on standard causal language modeling where:\n","- Input Processing:\n","The Shakespeare text is split into chunks of 512 tokens (SEQUENCE_LENGTH)\n","Each chunk is independent, with no overlap between chunks\n","Only chunks with more than 10 characters are kept\n","- Label Generation:\n","The DataCollatorForLanguageModeling(mlm=False) automatically handles label creation\n","For causal language modeling, it uses the input sequence itself as input\n","The labels are the same sequence but shifted right by one position\n","This creates the standard \"predict next token\" objective\n","- Text Handling:\n","The current implementation uses hard cutoffs at SEQUENCE_LENGTH\n","It doesn't use a sliding window approach (which would have overlapping chunks)\n","This means context is limited to the chunk boundaries\n","\n","\n","This approach is simple but effective for pretraining. Each batch contains independent chunks of Shakespeare text, and the model learns to predict the next token given the preceding tokens within each chunk.\n","\n","\n","If you wanted to improve this approach, you could implement:\n","- Sliding window chunking instead of hard cutoffs\n","- Preserving document boundaries\n","- Better handling of truncation to avoid cutting in the middle of sentences"],"metadata":{"id":"I5wCsOgYAarY"}},{"cell_type":"code","source":["#!/usr/bin/env python\n","# Pretraining script for SmolLMForCausalLM model on tiny Shakespeare dataset\n","\n","import os\n","import math\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from datasets import load_dataset\n","from transformers import (\n","    Trainer,\n","    TrainingArguments,\n","    DataCollatorForLanguageModeling,\n","    get_cosine_schedule_with_warmup,\n","    set_seed,\n",")\n","from transformers.trainer_pt_utils import get_parameter_names\n","from accelerate import Accelerator\n","from typing import Dict, List, Optional, Union, Any, Tuple\n","import gc\n","import logging\n","import sys\n","\n","# Import the custom model implementation\n","from smol_model import SmolLMConfig, SmolLMForCausalLM\n","\n","# Configure logging\n","logging.basicConfig(\n","    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n","    datefmt=\"%m/%d/%Y %H:%M:%S\",\n","    handlers=[logging.StreamHandler(sys.stdout)],\n","    level=logging.INFO,\n",")\n","logger = logging.getLogger(__name__)\n","\n","# Set seed for reproducibility\n","set_seed(42)\n","\n","# Constants\n","MODEL_NAME = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n","SEQUENCE_LENGTH = 512\n","BATCH_SIZE = 16  # Per-device batch size\n","GRADIENT_ACCUMULATION_STEPS = 4  # Adjust to achieve effective batch size of 0.5M tokens\n","LEARNING_RATE = 6e-4\n","WEIGHT_DECAY = 0.1\n","WARMUP_STEPS = 100\n","NUM_TRAIN_EPOCHS = 1\n","LOGGING_STEPS = 10\n","SAVE_STEPS = 1000\n","USE_FLASH_ATTENTION = True  # Whether to use Flash Attention\n","\n","\n","# Dataset preparation\n","def get_shakespeare_dataset():\n","    \"\"\"Load and prepare the tiny Shakespeare dataset for language modeling.\"\"\"\n","    # Direct download from raw.githubusercontent if dataset not available\n","    dataset_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n","\n","    # Try loading from HF datasets first\n","    try:\n","        logger.info(\"Attempting to load Shakespeare dataset from Hugging Face...\")\n","        dataset = load_dataset(\"tiny_shakespeare\")\n","        return dataset\n","    except:\n","        # If not available, download and process manually\n","        logger.info(f\"Downloading Shakespeare dataset from {dataset_url}...\")\n","        import requests\n","\n","        text = requests.get(dataset_url).text\n","\n","        # Create a simple text dataset\n","        train_split = int(len(text) * 0.9)\n","        train_text = text[:train_split]\n","        val_text = text[train_split:]\n","\n","        # Convert to HF dataset format\n","        from datasets import Dataset\n","\n","        train_dataset = Dataset.from_dict({\"text\": [train_text]})\n","        val_dataset = Dataset.from_dict({\"text\": [val_text]})\n","\n","        # Tokenize the dataset\n","        def tokenize_function(examples):\n","            tokenized = []\n","            for text in examples[\"text\"]:\n","                # Create chunks of SEQUENCE_LENGTH\n","                for i in range(0, len(text), SEQUENCE_LENGTH):\n","                    chunk = text[i : i + SEQUENCE_LENGTH]\n","                    if len(chunk) > 10:  # Only keep chunks with meaningful content\n","                        tokenized.append(chunk)\n","            return {\"text\": tokenized}\n","\n","        train_dataset = train_dataset.map(\n","            tokenize_function, batched=True, remove_columns=[\"text\"]\n","        )\n","        val_dataset = val_dataset.map(\n","            tokenize_function, batched=True, remove_columns=[\"text\"]\n","        )\n","\n","        return {\"train\": train_dataset, \"validation\": val_dataset}\n","\n","\n","# Model initialization with random weights\n","def init_model_for_training():\n","    \"\"\"Initialize the model with random weights for pretraining, with optional Flash Attention.\"\"\"\n","    from transformers import AutoConfig\n","\n","    logger.info(\"Initializing SmolLM model with random weights...\")\n","\n","    # Load the original configuration for architecture details\n","    official_config = AutoConfig.from_pretrained(MODEL_NAME, trust_remote_code=True)\n","\n","    # Create our custom config with Flash Attention enabled if requested\n","    custom_config = SmolLMConfig(\n","        official_config=official_config, use_flashattention=USE_FLASH_ATTENTION\n","    )\n","\n","    # Initialize our custom model\n","    logger.info(\n","        f\"Flash Attention is {'enabled' if USE_FLASH_ATTENTION else 'disabled'}\"\n","    )\n","    model = SmolLMForCausalLM(custom_config)\n","\n","    # Apply proper weight initialization for a transformer model\n","    logger.info(\"Applying weight initialization...\")\n","\n","    def _init_weights(module):\n","        if isinstance(module, nn.Linear):\n","            # Use standard initialization for linear layers\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        elif isinstance(module, nn.Embedding):\n","            # Initialize embeddings with normal distribution\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","            if hasattr(module, \"padding_idx\") and module.padding_idx is not None:\n","                with torch.no_grad():\n","                    module.weight[module.padding_idx].fill_(0)\n","\n","    # Apply the initialization to all modules\n","    model.apply(_init_weights)\n","\n","    # Apply special scaling to final layer norm\n","    with torch.no_grad():\n","        if hasattr(model.model, \"norm\"):\n","            model.model.norm.weight.fill_(1.0)\n","\n","    # Tie weights if specified in config\n","    if model.config.tie_word_embeddings:\n","        model.tie_weights()\n","        logger.info(\"Tied input and output embedding weights\")\n","\n","    # Use torch.compile to speed up training if available\n","    if torch.cuda.is_available() and hasattr(torch, \"compile\"):\n","        logger.info(\"Applying torch.compile for faster training...\")\n","        model = torch.compile(model)\n","\n","    return model\n","\n","\n","# Custom Optimizer with AdamW (fused), weight decay exclusion, and gradient norm clipping\n","def get_optimizer(model, lr, weight_decay):\n","    \"\"\"Create optimizer with proper weight decay exclusion and use fused Adam if available.\"\"\"\n","    # Filter parameters that should not have weight decay\n","    decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n","    decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n","\n","    # Organize parameters\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n","            \"weight_decay\": weight_decay,\n","        },\n","        {\n","            \"params\": [\n","                p for n, p in model.named_parameters() if n not in decay_parameters\n","            ],\n","            \"weight_decay\": 0.0,\n","        },\n","    ]\n","\n","    # Use fused Adam if available (faster on CUDA)\n","    if torch.cuda.is_available():\n","        try:\n","            from torch.optim.adam import Adam as FusedAdam\n","\n","            logger.info(\"Using fused Adam optimizer\")\n","            optimizer = FusedAdam(\n","                optimizer_grouped_parameters,\n","                lr=lr,\n","                betas=(0.9, 0.95),\n","                eps=1e-8,\n","                fused=True,\n","            )\n","        except ImportError:\n","            logger.info(\"Fused Adam not available, using standard AdamW\")\n","            optimizer = torch.optim.AdamW(\n","                optimizer_grouped_parameters,\n","                lr=lr,\n","                betas=(0.9, 0.95),\n","                eps=1e-8,\n","            )\n","    else:\n","        optimizer = torch.optim.AdamW(\n","            optimizer_grouped_parameters,\n","            lr=lr,\n","            betas=(0.9, 0.95),\n","            eps=1e-8,\n","        )\n","\n","    return optimizer\n","\n","\n","def main():\n","    # 1. Initialize accelerator\n","    accelerator = Accelerator(\n","        mixed_precision=\"fp16\", log_with=\"tensorboard\", project_dir=\"./logs\"\n","    )\n","\n","    accelerator.print(f\"Running on {accelerator.device}\")\n","\n","    # 2. Get dataset\n","    dataset = get_shakespeare_dataset()\n","\n","    # 3. Initialize model\n","    model = init_model_for_training()\n","\n","    # 4. Create tokenizer from model config\n","    tokenizer = model.get_input_embeddings()\n","\n","    # 5. Create data collator\n","    data_collator = DataCollatorForLanguageModeling(\n","        tokenizer=tokenizer,\n","        mlm=False,  # We're doing causal language modeling\n","    )\n","\n","    # 6. Prepare training arguments\n","    training_args = TrainingArguments(\n","        output_dir=\"./results\",\n","        overwrite_output_dir=True,\n","        num_train_epochs=NUM_TRAIN_EPOCHS,\n","        per_device_train_batch_size=BATCH_SIZE,\n","        per_device_eval_batch_size=BATCH_SIZE,\n","        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=SAVE_STEPS,\n","        save_strategy=\"steps\",\n","        save_steps=SAVE_STEPS,\n","        save_total_limit=2,\n","        learning_rate=LEARNING_RATE,\n","        weight_decay=WEIGHT_DECAY,\n","        warmup_steps=WARMUP_STEPS,\n","        lr_scheduler_type=\"cosine\",\n","        logging_dir=\"./logs\",\n","        logging_steps=LOGGING_STEPS,\n","        report_to=\"tensorboard\",\n","        fp16=True,  # Mixed precision training\n","        remove_unused_columns=False,\n","        dataloader_num_workers=4,\n","        gradient_checkpointing=True,  # Memory optimization\n","        max_grad_norm=1.0,  # Gradient norm clipping\n","    )\n","\n","    # 7. Create optimizer and scheduler\n","    optimizer = get_optimizer(model, LEARNING_RATE, WEIGHT_DECAY)\n","    total_steps = (\n","        len(dataset[\"train\"])\n","        // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)\n","        * NUM_TRAIN_EPOCHS\n","    )\n","    lr_scheduler = get_cosine_schedule_with_warmup(\n","        optimizer=optimizer,\n","        num_warmup_steps=WARMUP_STEPS,\n","        num_training_steps=total_steps,\n","    )\n","\n","    # 8. Create and start Trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=dataset[\"train\"],\n","        eval_dataset=dataset[\"validation\"],\n","        data_collator=data_collator,\n","        optimizers=(optimizer, lr_scheduler),\n","    )\n","\n","    # 9. Start training\n","    logger.info(\"Starting training...\")\n","    trainer.train()\n","\n","    # 10. Save final model\n","    trainer.save_model(\"./final_model\")\n","    logger.info(\"Training complete! Model saved to ./final_model\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"id":"2l4ia9W-BxBQ"},"execution_count":null,"outputs":[]}]}