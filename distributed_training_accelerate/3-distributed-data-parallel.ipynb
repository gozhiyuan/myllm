{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyP0H8RcDrOyjD2uPOVqPzpA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"s51CwoLRtHIy"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["## Distributed Data Parallel (DDP) Process\n","- Step 1: Multiple processes are launched, each loading data and model separately.\n","- Step 2: Each process performs forward propagation independently.\n","- Step 3: Each process computes the loss and performs backpropagation.\n","- Step 4: Gradients are synchronized across GPUs using all-reduce.\n","- Step 5: Each process updates its model weights with the synchronized gradients.\n","\n","\n","Compared to DataParallel, DDP eliminates GIL issues, distributes the computation more evenly, reduces synchronization overhead, and supports multi-node training."],"metadata":{"id":"RxKPnPpRtdC2"}},{"cell_type":"markdown","source":["## Basic Concepts in Distributed Data Parallel (DDP)\n","\n","1. **Group**: A collection of processes involved in a distributed training task. Typically, all GPUs participate in a single group.\n","\n","2. **World Size**: The total number of processes participating in the distributed training. Usually equal to the number of GPUs used.\n","\n","3. **Node**: A machine or container running the training. Each node can contain multiple GPUs.\n","\n","4. **Rank (Global Rank)**: The unique identifier assigned to each process in the distributed training. Used for communication and coordination.\n","\n","5. **Local Rank**: The rank of a process within a node. Helps identify which GPU a process should use on a given node.\n","\n","6. **Backend**: The communication protocol used for inter-process communication (e.g., NCCL, Gloo, MPI).  \n","   - **NCCL**: Optimized for GPU-to-GPU communication.  \n","   - **Gloo**: Supports both CPU and GPU training.  \n","   - **MPI**: Common in HPC environments.\n","\n","7. **All-Reduce**: The operation used to synchronize gradients across all processes. Each process computes its own gradients, and `all-reduce` ensures they are averaged and shared.\n","\n","8. **Broadcast**: Used to distribute initial model weights from rank 0 to all other processes.\n","\n","9. **Synchronization**: Ensures that all GPUs have the same model parameters and gradients after each update.\n","\n","10. **Gradient Bucketing**: Groups small gradient tensors together to optimize communication efficiency, reducing overhead.\n"],"metadata":{"id":"fxHdWpkXtZ17"}},{"cell_type":"code","source":["%%write ddp.py\n","\n","# %% [markdown]\n","# # 文本分类实例\n","\n","# %% [markdown]\n","# ## Step1 导入相关包\n","\n","# %%\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertTokenizer, BertForSequenceClassification\n","\n","import torch.distributed as dist\n","\n","dist.init_process_group(backend=\"nccl\")\n","\n","# %% [markdown]\n","# ## Step2 加载数据\n","\n","# %%\n","import pandas as pd\n","\n","data = pd.read_csv(\"./ChnSentiCorp_htl_all.csv\")\n","data\n","\n","# %%\n","data = data.dropna()\n","data\n","\n","# %% [markdown]\n","# ## Step3 创建Dataset\n","\n","# %%\n","from torch.utils.data import Dataset\n","\n","class MyDataset(Dataset):\n","\n","    def __init__(self) -> None:\n","        super().__init__()\n","        self.data = pd.read_csv(\"./ChnSentiCorp_htl_all.csv\")\n","        self.data = self.data.dropna()\n","\n","    def __getitem__(self, index):\n","        return self.data.iloc[index][\"review\"], self.data.iloc[index][\"label\"]\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","# %%\n","dataset = MyDataset()\n","# for i in range(5):\n","#     print(dataset[i])\n","\n","# %% [markdown]\n","# ## Step4 划分数据集\n","\n","# %%\n","import torch\n","from torch.utils.data import random_split\n","\n","\n","trainset, validset = random_split(dataset, lengths=[0.9, 0.1], generator=torch.Generator().manual_seed(42))\n","len(trainset), len(validset)\n","\n","# %%\n","for i in range(5):\n","    print(trainset[i])\n","\n","# %% [markdown]\n","# ## Step5 创建Dataloader\n","\n","# %%\n","import torch\n","\n","tokenizer = BertTokenizer.from_pretrained(\"/gemini/code/model\")\n","\n","def collate_func(batch):\n","    texts, labels = [], []\n","    for item in batch:\n","        texts.append(item[0])\n","        labels.append(item[1])\n","    inputs = tokenizer(texts, max_length=128, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n","    inputs[\"labels\"] = torch.tensor(labels)\n","    return inputs\n","\n","# %%\n","from torch.utils.data import DataLoader\n","from torch.utils.data.distributed import DistributedSampler\n","\n","trainloader = DataLoader(trainset, batch_size=32, collate_fn=collate_func, sampler=DistributedSampler(trainset))\n","validloader = DataLoader(validset, batch_size=64, collate_fn=collate_func, sampler=DistributedSampler(validset))\n","\n","# %%\n","next(enumerate(validloader))[1]\n","\n","# %% [markdown]\n","# ## Step6 创建模型及优化器\n","\n","# %%\n","from torch.optim import Adam\n","import os\n","from torch.nn.parallel import DistributedDataParallel as DDP\n","\n","model = BertForSequenceClassification.from_pretrained(\"/gemini/code/model\")\n","\n","if torch.cuda.is_available():\n","    model = model.to(int(os.environ[\"LOCAL_RANK\"]))\n","\n","model = DDP(model)\n","\n","# %%\n","optimizer = Adam(model.parameters(), lr=2e-5)\n","\n","# %% [markdown]\n","# ## Step7 训练与验证\n","\n","def print_rank_0(info):\n","    if int(os.environ[\"RANK\"]) == 0:\n","        print(info)\n","\n","# %%\n","def evaluate():\n","    model.eval()\n","    acc_num = 0\n","    with torch.inference_mode():\n","        for batch in validloader:\n","            if torch.cuda.is_available():\n","                batch = {k: v.to(int(os.environ[\"LOCAL_RANK\"])) for k, v in batch.items()}\n","            output = model(**batch)\n","            pred = torch.argmax(output.logits, dim=-1)\n","            acc_num += (pred.long() == batch[\"labels\"].long()).float().sum()\n","    dist.all_reduce(acc_num)\n","    return acc_num / len(validset)\n","\n","def train(epoch=3, log_step=100):\n","    global_step = 0\n","    for ep in range(epoch):\n","        model.train()\n","        trainloader.sampler.set_epoch(ep)\n","        for batch in trainloader:\n","            if torch.cuda.is_available():\n","                batch = {k: v.to(int(os.environ[\"LOCAL_RANK\"])) for k, v in batch.items()}\n","            optimizer.zero_grad()\n","            output = model(**batch)\n","            loss = output.loss\n","            loss.backward()\n","            optimizer.step()\n","            if global_step % log_step == 0:\n","                dist.all_reduce(loss, op=dist.ReduceOp.AVG)\n","                print_rank_0(f\"ep: {ep}, global_step: {global_step}, loss: {loss.item()}\")\n","            global_step += 1\n","        acc = evaluate()\n","        print_rank_0(f\"ep: {ep}, acc: {acc}\")\n","\n","# %% [markdown]\n","# ## Step8 模型训练\n","\n","# %%\n","train()"],"metadata":{"id":"v1PgG24FuQD7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.distributed as dist\n","import torch.multiprocessing as mp\n","from torch.utils.data import DataLoader, Dataset, random_split\n","from torch.utils.data.distributed import DistributedSampler\n","from torch.nn.parallel import DistributedDataParallel as DDP\n","from torch.optim import Adam\n","from torchtext.datasets import YelpReviewFull\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","import os\n","\n","# Initialize distributed process group for multi-GPU training\n","dist.init_process_group(backend=\"nccl\")  # Using NCCL for efficient GPU communication\n","\n","# Load tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    \"google-bert/bert-base-cased\", num_labels=5, torch_dtype=\"auto\"\n",")\n","\n","# Move model to the appropriate device based on the process rank\n","if torch.cuda.is_available():\n","    model = model.to(int(os.environ[\"LOCAL_RANK\"]))\n","\n","# Wrap the model with DistributedDataParallel for synchronized multi-GPU training\n","model = DDP(model)\n","\n","# Custom Dataset class to handle YelpReviewFull dataset\n","class YelpDataset(Dataset):\n","    def __init__(self, split=\"train\"):\n","        super().__init__()\n","        self.data = list(YelpReviewFull(split=split))  # Load dataset into memory\n","\n","    def __getitem__(self, index):\n","        return self.data[index][1], self.data[index][0] - 1  # Adjust labels to be 0-based\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","# Create dataset instances\n","# Since we're using the YelpReviewFull dataset and directly selecting the train and test splits,\n","# We no longer need to use random_split() with a fixed seed.\n","# The dataset is already predefined, ensuring consistency across GPUs.\n","train_dataset = YelpDataset(split=\"train\")\n","valid_dataset = YelpDataset(split=\"test\")\n","\n","# Function to preprocess batches (tokenization, padding, and conversion to tensors)\n","def collate_fn(batch):\n","    texts, labels = zip(*batch)\n","    inputs = tokenizer(list(texts), max_length=128, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n","    inputs[\"labels\"] = torch.tensor(labels)\n","    return inputs\n","\n","# Define DistributedSampler to ensure proper data shuffling across processes\n","train_sampler = DistributedSampler(train_dataset)\n","valid_sampler = DistributedSampler(valid_dataset)\n","\n","# Create DataLoaders for training and validation\n","train_loader = DataLoader(train_dataset, batch_size=32, collate_fn=collate_fn, sampler=train_sampler)\n","valid_loader = DataLoader(valid_dataset, batch_size=64, collate_fn=collate_fn, sampler=valid_sampler)\n","\n","# Optimizer setup\n","optimizer = Adam(model.parameters(), lr=2e-5)\n","\n","# Function to ensure only rank 0 prints log messages\n","def print_rank_0(info):\n","    if int(os.environ.get(\"RANK\", 0)) == 0:\n","        print(info)\n","\n","# Evaluation function\n","def evaluate():\n","    model.eval()\n","    acc_num = torch.tensor(0.0, device=model.device)\n","    with torch.inference_mode():\n","        for batch in valid_loader:\n","            if torch.cuda.is_available():\n","                batch = {k: v.to(model.device) for k, v in batch.items()}\n","            output = model(**batch)\n","            pred = torch.argmax(output.logits, dim=-1)\n","            acc_num += (pred.long() == batch[\"labels\"].long()).float().sum()\n","\n","    # Synchronize accuracy across all GPUs\n","    dist.all_reduce(acc_num, op=dist.ReduceOp.SUM)\n","    return acc_num.item() / len(valid_dataset)\n","\n","# Training function\n","def train(epochs=3, log_step=100):\n","    global_step = 0\n","    for epoch in range(epochs):\n","        model.train()\n","        train_loader.sampler.set_epoch(epoch)  # Ensure proper shuffling across epochs\n","        for batch in train_loader:\n","            if torch.cuda.is_available():\n","                batch = {k: v.to(model.device) for k, v in batch.items()}\n","\n","            optimizer.zero_grad()\n","            output = model(**batch)\n","            loss = output.loss\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Log training progress on rank 0\n","            if global_step % log_step == 0:\n","                loss_item = loss.detach().clone()\n","                dist.all_reduce(loss_item, op=dist.ReduceOp.AVG)  # Sync loss across GPUs\n","                print_rank_0(f\"Epoch: {epoch}, Step: {global_step}, Loss: {loss_item.item()}\")\n","            global_step += 1\n","\n","        # Evaluate the model after each epoch\n","        acc = evaluate()\n","        print_rank_0(f\"Epoch: {epoch}, Accuracy: {acc}\")\n","\n","# Start training\n","train()\n","\n","dist.destroy_process_group()  # Cleanup distributed processes\n"],"metadata":{"id":"-LWe-jtMtcMu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !torchrun --nproc_per_node=2 ddp.py"],"metadata":{"id":"TYheA19b1IGR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Note\n","\n","If your dataset cannot be evenly split across GPUs, `DistributedSampler` automatically handles this by assigning different numbers of samples to different GPUs as needed. However, it does not pad the dataset. Instead, it may drop some samples if `drop_last=True` (which can be set in the `DataLoader`), or it may allow an uneven last batch.\n","\n","Solutions:\n","1. **Allow smaller last batches (default behavior):**\n","   - If `drop_last=False` (default), the last batch on some GPUs may have fewer samples.\n","\n","2. **Ensure equal data distribution with padding:**\n","   - You can modify `collate_fn` to pad the dataset artificially so all GPUs get an equal number of samples.\n","\n","3. **Use `DistributedSampler` with `num_replicas`:**\n","   - If you manually set `num_replicas`, you can control how data is split across GPUs.\n"],"metadata":{"id":"VqA5i3UG5RgI"}},{"cell_type":"markdown","source":["## DDP Training with Trainer:\n","For DDP with `Trainer`, you can simply enable multi-GPU training using the `TrainingArguments`. Below are the essential components for setting up DDP:\n","\n","### Distributed Training:\n","- When training on multiple GPUs, you can set `per_device_train_batch_size` based on how many GPUs you have available.\n","- By default, `Trainer` handles distributed data parallelism, ensuring each GPU gets a portion of the dataset.\n","\n","### Setting up DDP:\n","- DDP is enabled automatically if you are using `Trainer` with distributed environment variables (like `CUDA_VISIBLE_DEVICES` or setting the number of GPUs with `--nproc_per_node` when launching the script).\n","\n","### Trainer in Distributed Mode:\n","- If running the script using `torchrun` or `accelerate`, DDP will be enabled automatically. The `Trainer` will manage distributing the dataset and model across the GPUs.\n","- When using DDP, batch size will be split across GPUs, and each GPU computes gradients independently.\n"],"metadata":{"id":"C5r_yHdq9qJi"}},{"cell_type":"code","source":["from datasets import load_dataset\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n","from transformers import DataCollatorWithPadding\n","import numpy as np\n","import evaluate\n","\n","# Load the dataset\n","dataset = load_dataset(\"yelp_review_full\")\n","print(f\"Example from dataset: {dataset['train'][100]}\")\n","\n","# Initialize tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n","\n","# Tokenize the dataset\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n","\n","tokenized_datasets = dataset.map(tokenize_function, batched=True)\n","print(f\"Tokenized dataset: {tokenized_datasets}\")\n","\n","# Subsample for quick experimentation\n","small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(100))\n","small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(50))\n","\n","# Load the model\n","model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5, torch_dtype=\"auto\")\n","print(f\"Model config: {model.config}\")\n","\n","# Metrics to compute during training\n","acc_metric = evaluate.load(\"accuracy\")\n","f1_metric = evaluate.load(\"f1\")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    acc = acc_metric.compute(predictions=predictions, references=labels)\n","    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n","    acc.update(f1)\n","    return acc\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"test_trainer\",                          # Output directory for model checkpoints\n","    per_device_train_batch_size=8,                      # Batch size per device during training\n","    per_device_eval_batch_size=16,                      # Batch size per device during evaluation\n","    logging_steps=10,                                   # Number of steps between logging\n","    evaluation_strategy=\"epoch\",                        # Evaluate at the end of each epoch\n","    save_strategy=\"epoch\",                              # Save checkpoints at the end of each epoch\n","    num_train_epochs=4,                                 # Number of training epochs\n","    save_total_limit=3,                                 # Maximum number of saved checkpoints\n","    learning_rate=2e-5,                                 # Learning rate\n","    weight_decay=0.01,                                  # Weight decay for regularization\n","    metric_for_best_model=\"f1\",                         # Metric to monitor for best model\n","    load_best_model_at_end=True,                        # Load the best model after training\n",")\n","\n","# Initialize the Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=small_train_dataset,\n","    eval_dataset=small_eval_dataset,\n","    compute_metrics=compute_metrics,\n",")\n","\n","# Start training\n","trainer.train()\n"],"metadata":{"id":"lm_5sc5l5nmK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !torchrun --nproc_per_node=2 ddp_trainer.py"],"metadata":{"id":"zlfvKL1T9UzX"},"execution_count":null,"outputs":[]}]}