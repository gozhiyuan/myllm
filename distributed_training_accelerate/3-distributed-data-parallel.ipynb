{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s51CwoLRtHIy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxKPnPpRtdC2"
   },
   "source": [
    "## Distributed Data Parallel (DDP) Process\n",
    "- Step 1: Multiple processes are launched, each loading data and model separately.\n",
    "- Step 2: Each process performs forward propagation independently.\n",
    "- Step 3: Each process computes the loss and performs backpropagation.\n",
    "- Step 4: Gradients are synchronized across GPUs using all-reduce.\n",
    "- Step 5: Each process updates its model weights with the synchronized gradients.\n",
    "\n",
    "\n",
    "Compared to DataParallel, DDP eliminates GIL issues, distributes the computation more evenly, reduces synchronization overhead, and supports multi-node training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxHdWpkXtZ17"
   },
   "source": [
    "## Basic Concepts in Distributed Data Parallel (DDP)\n",
    "\n",
    "1. **Group**: A collection of processes involved in a distributed training task. Typically, all GPUs participate in a single group.\n",
    "\n",
    "2. **World Size**: The total number of processes participating in the distributed training. Usually equal to the number of GPUs used.\n",
    "\n",
    "3. **Node**: A machine or container running the training. Each node can contain multiple GPUs.\n",
    "\n",
    "4. **Rank (Global Rank)**: The unique identifier assigned to each process in the distributed training. Used for communication and coordination.\n",
    "\n",
    "5. **Local Rank**: The rank of a process within a node. Helps identify which GPU a process should use on a given node.\n",
    "\n",
    "6. **Backend**: The communication protocol used for inter-process communication (e.g., NCCL, Gloo, MPI).  \n",
    "   - **NCCL**: Optimized for GPU-to-GPU communication.  \n",
    "   - **Gloo**: Supports both CPU and GPU training.  \n",
    "   - **MPI**: Common in HPC environments.\n",
    "\n",
    "7. **All-Reduce**: The operation used to synchronize gradients across all processes. Each process computes its own gradients, and `all-reduce` ensures they are averaged and shared.\n",
    "\n",
    "8. **Broadcast**: Used to distribute initial model weights from rank 0 to all other processes.\n",
    "\n",
    "9. **Synchronization**: Ensures that all GPUs have the same model parameters and gradients after each update.\n",
    "\n",
    "10. **Gradient Bucketing**: Groups small gradient tensors together to optimize communication efficiency, reducing overhead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-LWe-jtMtcMu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ddp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ddp.py\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim import Adam\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Initialize the distributed process group\n",
    "dist.init_process_group(backend=\"nccl\")  # or \"gloo\" if you're using CPU\n",
    "\n",
    "# Load the dataset from Hugging Face\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"google-bert/bert-base-cased\", num_labels=5, torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "# Set device based on LOCAL_RANK environment variable (set by torchrun)\n",
    "local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "device = torch.device(\"cuda\", local_rank) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Wrap the model with DistributedDataParallel\n",
    "model = DDP(model, device_ids=[local_rank] if torch.cuda.is_available() else None)\n",
    "\n",
    "# Define a custom Dataset class\n",
    "class YelpReviewDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer):\n",
    "        self.dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        text = item['text']\n",
    "        label = item['label']\n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',  # fixed length padding; for dynamic padding, use a collate_fn\n",
    "            max_length=128,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        # Remove the extra batch dimension\n",
    "        output = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        output[\"labels\"] = torch.tensor(label, dtype=torch.long)\n",
    "        return output\n",
    "\n",
    "# Create Dataset instances for train and test splits\n",
    "train_dataset = YelpReviewDataset(dataset['train'], tokenizer)\n",
    "test_dataset = YelpReviewDataset(dataset['test'], tokenizer)\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Randomly select 100 samples for training and 50 samples for testing\n",
    "num_train_samples = 1000\n",
    "num_test_samples = 500\n",
    "\n",
    "train_indices = torch.randperm(len(train_dataset)).tolist()[:num_train_samples]\n",
    "test_indices = torch.randperm(len(test_dataset)).tolist()[:num_test_samples]\n",
    "\n",
    "# Create subsets of the original datasets\n",
    "train_subset = Subset(train_dataset, train_indices)\n",
    "test_subset = Subset(test_dataset, test_indices)\n",
    "\n",
    "# Create DistributedSamplers for the subsets\n",
    "train_sampler = DistributedSampler(train_subset, shuffle=True)\n",
    "test_sampler = DistributedSampler(test_subset, shuffle=False)\n",
    "\n",
    "# Create DataLoaders using the DistributedSamplers\n",
    "train_loader = DataLoader(train_subset, sampler=train_sampler, batch_size=32)\n",
    "test_loader = DataLoader(test_subset, sampler=test_sampler, batch_size=32)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "def train(epochs=3):\n",
    "    global_step = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        # Set the epoch for the sampler to ensure a different shuffling each epoch\n",
    "        train_sampler.set_epoch(epoch)\n",
    "        for batch in train_loader:\n",
    "            # Move all batch tensors to the correct device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if global_step % 10 == 0 and dist.get_rank() == 0:\n",
    "                print(f\"Epoch: {epoch}, Step: {global_step}, Loss: {loss.item()}\")\n",
    "            global_step += 1\n",
    "\n",
    "    # # Save the model checkpoint only on the main process\n",
    "    # if dist.get_rank() == 0:\n",
    "    #     torch.save(model.state_dict(), \"model_checkpoint.pth\")\n",
    "    #     print(\"Model checkpoint saved.\")\n",
    "\n",
    "    # Cleanup distributed processes\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYheA19b1IGR"
   },
   "outputs": [],
   "source": [
    "# !torchrun --nproc_per_node=2 ddp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqA5i3UG5RgI"
   },
   "source": [
    "## Handling Uneven Dataset Sizes in Distributed Training\n",
    "\n",
    "When using PyTorch's `DistributedSampler` in a distributed training setup, it's important to manage datasets that aren't perfectly divisible by the number of GPUs (or processes). Here's how `DistributedSampler` addresses this scenario:\n",
    "\n",
    "1. **Automatic Handling of Non-Divisible Datasets:**\n",
    "   - If the dataset size isn't divisible by the number of replicas (`num_replicas`), `DistributedSampler` can pad the dataset with extra samples to ensure each GPU receives an equal number of samples. This padding is controlled by the `drop_last` parameter.\n",
    "     - `drop_last=False` (default): Pads the dataset with extra samples, allowing all GPUs to process the same number of samples.\n",
    "     - `drop_last=True`: Drops the last incomplete batch, which can lead to some samples being omitted from training.\n",
    "\n",
    "2. **Managing the Last Batch:**\n",
    "   - With `drop_last=False`, the last batch might be smaller, leading to potential inefficiencies. To address this:\n",
    "     - **Allow Smaller Last Batches:** Accept the default behavior where the last batch may have fewer samples.\n",
    "     - **Ensure Equal Data Distribution with Padding:** Modify the `collate_fn` to pad the last batch, ensuring all batches have the same size.\n",
    "\n",
    "3. **Manual Control with `num_replicas`:**\n",
    "   - By setting the `num_replicas` parameter manually, you can control how the dataset is split across GPUs. This is useful in scenarios where the number of processes differs from the number of available GPUs.\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "- **Allow Smaller Last Batches (Default Behavior):**\n",
    "  - With `drop_last=False`, the last batch on some GPUs may have fewer samples. This approach is straightforward but might lead to inefficiencies if the imbalance is significant.\n",
    "\n",
    "- **Ensure Equal Data Distribution with Padding:**\n",
    "  - Customize the `collate_fn` in your `DataLoader` to pad batches so that all GPUs process batches of equal size. This can help maintain efficiency and consistency across GPUs.\n",
    "\n",
    "- **Use `DistributedSampler` with `num_replicas`:**\n",
    "  - Manually setting `num_replicas` allows you to define how data is distributed across GPUs, providing finer control over the training process.\n",
    "\n",
    "By understanding and implementing these strategies, you can effectively manage uneven datasets in distributed training environments, ensuring efficient and balanced workload distribution across all GPUs. [Reference](https://github.com/pytorch/pytorch/issues/49180?utm_source=chatgpt.com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5r_yHdq9qJi"
   },
   "source": [
    "## DDP Training with Trainer:\n",
    "For DDP with `Trainer`, you can simply enable multi-GPU training using the `TrainingArguments`. Below are the essential components for setting up DDP:\n",
    "\n",
    "### Distributed Training:\n",
    "- When training on multiple GPUs, you can set `per_device_train_batch_size` based on how many GPUs you have available.\n",
    "- By default, `Trainer` handles distributed data parallelism, ensuring each GPU gets a portion of the dataset.\n",
    "\n",
    "### Setting up DDP:\n",
    "- DDP is enabled automatically if you are using `Trainer` with distributed environment variables (like `CUDA_VISIBLE_DEVICES` or setting the number of GPUs with `--nproc_per_node` when launching the script).\n",
    "\n",
    "### Trainer in Distributed Mode:\n",
    "- If running the script using `torchrun` or `accelerate`, DDP will be enabled automatically. The `Trainer` will manage distributing the dataset and model across the GPUs.\n",
    "- When using DDP, batch size will be split across GPUs, and each GPU computes gradients independently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lm_5sc5l5nmK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ddp_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ddp_trainer.py\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "# print(f\"Example from dataset: {dataset['train'][100]}\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "\n",
    "# Subsample for quick experimentation\n",
    "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = dataset[\"test\"].shuffle(seed=42).select(range(500))\n",
    "\n",
    "# Tokenize the subsampled datasets\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_train_dataset = small_train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval_dataset = small_eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Initialize the data collator for dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"google-bert/bert-base-cased\", num_labels=5, torch_dtype=\"auto\"\n",
    ")\n",
    "print(f\"Model config: {model.config}\")\n",
    "\n",
    "# Metrics to compute during training\n",
    "acc_metric = evaluate.load(\"metric_accuracy.py\")\n",
    "f1_metric = evaluate.load(\"metric_f1.py\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    acc.update(f1)\n",
    "    return acc\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",                          # Output directory for model checkpoints\n",
    "    per_device_train_batch_size=32,                     # Batch size per device during training\n",
    "    per_device_eval_batch_size=32,                      # Batch size per device during evaluation\n",
    "    logging_steps=10,                                   # Number of steps between logging\n",
    "    evaluation_strategy=\"epoch\",                        # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",                              # Save checkpoints at the end of each epoch\n",
    "    num_train_epochs=4,                                 # Number of training epochs\n",
    "    save_total_limit=3,                                 # Maximum number of saved checkpoints\n",
    "    learning_rate=2e-5,                                 # Learning rate\n",
    "    weight_decay=0.01,                                  # Weight decay for regularization\n",
    "    metric_for_best_model=\"f1\",                         # Metric to monitor for best model\n",
    "    load_best_model_at_end=True,                        # Load the best model after training\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zlfvKL1T9UzX"
   },
   "outputs": [],
   "source": [
    "# !torchrun --nproc_per_node=2 ddp_trainer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compare two approaches for distributed training using the Yelp Review Full dataset: a custom PyTorch DDP implementation and an approach using the HuggingFace Trainer. Below are the key implementation steps and considerations for each method.\n",
    "\n",
    "## Custom PyTorch DDP Implementation\n",
    "\n",
    "- **Dataset Preparation**\n",
    "  - Load the dataset using Hugging Face’s `load_dataset`.\n",
    "  - Create a custom `Dataset` class (e.g., `YelpReviewDataset`) that performs tokenization with `AutoTokenizer`.\n",
    "  - For quick experimentation, subsample the dataset (e.g., randomly selecting 100 training and 50 test samples) using functions like `torch.randperm` and `Subset`.\n",
    "\n",
    "- **DataLoader and Distributed Sampling**\n",
    "  - Use `DistributedSampler` on the subsampled datasets to split data across GPUs.\n",
    "  - Create DataLoaders that incorporate the sampler; optionally use a custom collate function or `DataCollatorWithPadding` for dynamic padding.\n",
    "  \n",
    "- **Distributed Training Setup**\n",
    "  - Initialize the process group (e.g., with NCCL) for multi-GPU training.\n",
    "  - Assign each process to a GPU using environment variables (like `LOCAL_RANK`).\n",
    "  - Wrap the model in `DistributedDataParallel` (DDP) to synchronize gradient updates.\n",
    "  - Implement a custom training loop that handles forward passes, loss computation, backpropagation, and optimizer steps.\n",
    "  - Save checkpoints from the main process (rank 0) to avoid conflicts.\n",
    "\n",
    "## HuggingFace Trainer Implementation\n",
    "\n",
    "- **Dataset Preparation and Tokenization**\n",
    "  - Load the dataset and immediately subsample the training and evaluation splits.\n",
    "  - Use the `map` function with a tokenization routine to process only the selected subsets.\n",
    "  - Employ `DataCollatorWithPadding` to enable dynamic padding within each batch, avoiding the overhead of fixed-length padding.\n",
    "\n",
    "- **Trainer Configuration**\n",
    "  - Load a pre-trained model (e.g., using `AutoModelForSequenceClassification`) and set up training parameters.\n",
    "  - Define `TrainingArguments` to specify batch sizes, learning rate, evaluation strategy, checkpoint saving, and more.\n",
    "  - Pass the tokenized datasets, data collator, and a custom metrics function to the `Trainer` class.\n",
    "  \n",
    "- **Training and Evaluation**\n",
    "  - The Trainer abstracts the training loop, automatically handling logging, evaluation, checkpointing, and distributed training setup.\n",
    "  - Distributed training is managed internally, so you don't need to explicitly initialize process groups or manage device assignments.\n",
    "\n",
    "## Key Differences and Considerations\n",
    "\n",
    "- **Flexibility vs. Abstraction**\n",
    "  - The custom PyTorch DDP approach offers complete control over each training step, which is ideal for custom or research-oriented projects.\n",
    "  - The HuggingFace Trainer provides a higher-level API that simplifies the training process, making it easier to prototype and experiment.\n",
    "\n",
    "- **Efficiency in Data Preparation**\n",
    "  - In the custom implementation, tokenizing the full dataset before sampling can be inefficient. An improvement is to sample first and then tokenize only the necessary subset.\n",
    "  - The Trainer approach leverages the dataset’s mapping functions to efficiently tokenize only the selected samples.\n",
    "\n",
    "- **Distributed Training Overhead**\n",
    "  - With custom DDP, you must manage the initialization of distributed processes, device assignments, and explicit saving of model checkpoints.\n",
    "  - The Trainer automates these tasks, reducing boilerplate code and minimizing potential errors in distributed settings.\n",
    "\n",
    "In summary, choose the custom DDP approach if you need detailed control over the training loop and data handling. Use the HuggingFace Trainer for rapid prototyping and when you prefer an out-of-the-box solution that handles distributed training with minimal configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP0H8RcDrOyjD2uPOVqPzpA",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
