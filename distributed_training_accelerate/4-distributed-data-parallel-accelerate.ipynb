{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jKjo3Lv-CJJe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6fcuxqECVbI"
   },
   "source": [
    "# Introduction to the `accelerate` Package\n",
    "\n",
    "The `accelerate` package, developed by Hugging Face, is designed to simplify and streamline the process of training and deploying PyTorch models across various hardware configurations, including multiple GPUs, TPUs, and distributed environments.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Unified API**: Provides a consistent interface for training and inference, abstracting away the complexities of different hardware setups.\n",
    "- **Automatic Device Placement**: Automatically places models and data on the appropriate device, eliminating the need for manual device management.\n",
    "- **Mixed Precision Training**: Supports mixed precision training to accelerate computations and reduce memory usage.\n",
    "- **Distributed Training**: Facilitates distributed training across multiple GPUs or TPUs with minimal code changes.\n",
    "\n",
    "## Installation\n",
    "\n",
    "To install the `accelerate` package, use pip:\n",
    "\n",
    "```bash\n",
    "pip install accelerate\n",
    "```\n",
    "\n",
    "\n",
    "## Basic Usage\n",
    "Here's a simple example of how to use accelerate in a PyTorch training loop:\n",
    "\n",
    "```python\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Initialize the accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Prepare your model, optimizer, and dataloaders\n",
    "model, optimizer, train_dataloader, scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, scheduler\n",
    ")\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    inputs, targets = batch\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_function(outputs, targets)\n",
    "    accelerator.backward(loss)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "```\n",
    "\n",
    "## Multi-GPU / Distributed Training\n",
    "\n",
    "### Running Multi-GPU Training:\n",
    "  - `accelerate launch --multi_gpu --num_processes 4 script.py`\n",
    "\n",
    "### Enable Distributed Training in Code:\n",
    "  - ```python\n",
    "    accelerator = Accelerator()\n",
    "    model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader)\n",
    "    ```\n",
    "  - `accelerator.prepare()` automatically adapts to DDP (DistributedDataParallel).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On-the-fly Tokenization with `collate_fn` VS Pre-tokenization\n",
    "\n",
    "In our previous implementation, tokenization was performed during dataset preprocessing, which meant that the entire dataset was tokenized and stored before training. Here we will use `collate_fn` for the tokenization. Both approaches can support dynamic padding, but they differ in when and how tokenization is applied. Here are the key pros and cons of each method:\n",
    "\n",
    "### Pre-tokenization During Dataset Mapping\n",
    "\n",
    "**How It Works:**  \n",
    "- You tokenize the entire dataset ahead of training using a mapping function (e.g., `dataset.map(tokenize_function, batched=True)`).\n",
    "- The tokenized data is stored and directly fed to the model during training.\n",
    "\n",
    "**Pros:**  \n",
    "- **Faster Training Loop:**  \n",
    "  - Since tokenization is done once, each training epoch simply loads preprocessed data, reducing per-epoch CPU overhead.\n",
    "- **Consistency:**  \n",
    "  - Each sample is tokenized only once, ensuring consistent input representations throughout training.\n",
    "- **Simpler DataLoader:**  \n",
    "  - The DataLoader simply collates already tokenized tensors, which can be easier to debug.\n",
    "\n",
    "**Cons:**  \n",
    "- **High Preprocessing Cost:**  \n",
    "  - Tokenizing the entire dataset upfront can be very time-consuming, especially for large datasets.\n",
    "- **Increased Storage Requirements:**  \n",
    "  - The pre-tokenized dataset occupies additional disk space.\n",
    "- **Less Flexibility:**  \n",
    "  - Adjustments to tokenization parameters require reprocessing the entire dataset.\n",
    "\n",
    "### On-the-fly Tokenization with `collate_fn`\n",
    "\n",
    "**How It Works:**  \n",
    "- The dataset’s `__getitem__` returns raw text and labels.\n",
    "- A custom `collate_fn` tokenizes and dynamically pads each batch at runtime.\n",
    "\n",
    "**Pros:**  \n",
    "- **Dynamic Padding Efficiency:**  \n",
    "  - Padding is computed based on the longest sequence in each batch, reducing wasted space compared to fixed-length padding.\n",
    "- **Reduced Preprocessing Time:**  \n",
    "  - Only a small subset (or batches) is tokenized during training, saving initial processing time and disk storage.\n",
    "- **Flexibility:**  \n",
    "  - Easily adjust tokenization parameters or perform additional processing on batches without reprocessing the whole dataset.\n",
    "- **Parallelized Processing:**  \n",
    "  - DataLoader workers can tokenize different batches in parallel, mitigating some of the on-the-fly overhead.\n",
    "\n",
    "**Cons:**  \n",
    "- **Increased CPU Overhead During Training:**  \n",
    "  - Tokenization is repeated for every batch in every epoch, which can slow down training if tokenization is computationally heavy.\n",
    "- **Potential Inconsistencies:**  \n",
    "  - Although minimal, tokenization performed on-the-fly may introduce slight variability across epochs if not carefully controlled.\n",
    "- **Debugging Complexity:**  \n",
    "  - Issues related to tokenization might be harder to isolate since the process is embedded within the batch collation step.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Pre-tokenization** is ideal when you want fast training iterations and can invest in a longer preprocessing stage with additional storage.  \n",
    "- **On-the-fly tokenization using `collate_fn`** offers greater flexibility and dynamic padding, which is beneficial for rapid prototyping and when working with large datasets where storage is a constraint.\n",
    "\n",
    "Choosing between these methods depends on your project's priorities: training speed versus preprocessing time and storage efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qSFt1YjTCVzT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ddp_accelerator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ddp_accelerator.py\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import Adam\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Initialize the Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"google-bert/bert-base-cased\", num_labels=5, torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "# Define a custom Dataset class\n",
    "class YelpReviewDataset(Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.dataset = dataset[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        text = item['text']\n",
    "        label = item['label']\n",
    "        return text, label\n",
    "\n",
    "# Instantiate the datasets\n",
    "train_dataset = YelpReviewDataset(split='train')\n",
    "test_dataset = YelpReviewDataset(split='test')\n",
    "\n",
    "# Function to create a random subset of the dataset\n",
    "def create_subset_indices(dataset, num_samples):\n",
    "    indices = list(range(len(dataset)))\n",
    "    random.seed(42)  # For reproducibility\n",
    "    random.shuffle(indices)\n",
    "    return indices[:num_samples]\n",
    "\n",
    "# Create subsets\n",
    "train_indices = create_subset_indices(train_dataset, 1000)\n",
    "test_indices = create_subset_indices(test_dataset, 500)\n",
    "\n",
    "train_subset = Subset(train_dataset, train_indices)\n",
    "test_subset = Subset(test_dataset, test_indices)\n",
    "\n",
    "# Define the collate function for tokenization\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    inputs = tokenizer(\n",
    "        list(texts),\n",
    "        max_length=128,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs[\"labels\"] = torch.tensor(labels)\n",
    "    return inputs\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_subset,\n",
    "    batch_size=32,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    test_subset,\n",
    "    batch_size=64,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Optimizer setup\n",
    "optimizer = Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(ddp_model, ddp_valid_loader):\n",
    "    ddp_model.eval()\n",
    "    acc_num = 0\n",
    "    with torch.inference_mode():\n",
    "        for batch in ddp_valid_loader:\n",
    "            output = ddp_model(**batch)\n",
    "            pred = torch.argmax(output.logits, dim=-1)\n",
    "            # Gather predictions and references from all processes\n",
    "            pred, refs = accelerator.gather_for_metrics((pred, batch[\"labels\"]))\n",
    "            acc_num += (pred.long() == refs.long()).float().sum()\n",
    "    return acc_num / len(ddp_valid_loader.dataset)\n",
    "\n",
    "# Training function with local variable renaming\n",
    "def train(epochs=3, log_step=10):\n",
    "    global_step = 0\n",
    "    # Prepare for distributed training, renaming the local variables\n",
    "    ddp_model, ddp_optimizer, ddp_train_loader, ddp_valid_loader = accelerator.prepare(\n",
    "        model, optimizer, train_loader, valid_loader\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        ddp_model.train()\n",
    "        for batch in ddp_train_loader:\n",
    "            ddp_optimizer.zero_grad()\n",
    "            output = ddp_model(**batch)\n",
    "            loss = output.loss\n",
    "            accelerator.backward(loss)\n",
    "            ddp_optimizer.step()\n",
    "\n",
    "            if global_step % log_step == 0:\n",
    "                loss_val = accelerator.reduce(loss, \"mean\")\n",
    "                accelerator.print(f\"Epoch: {epoch}, global_step: {global_step}, loss: {loss_val.item()}\")\n",
    "            global_step += 1\n",
    "\n",
    "        acc = evaluate(ddp_model, ddp_valid_loader)\n",
    "        accelerator.print(f\"Epoch: {epoch}, Accuracy: {acc}\")\n",
    "\n",
    "# Start training\n",
    "train()\n",
    "\n",
    "# Cleanup\n",
    "accelerator.end_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ExO8hoSCET2K"
   },
   "outputs": [],
   "source": [
    "# !torchrun --nproc_per_node=2 ddp_accelerator.py\n",
    "# !accelerate launch ddp_accelerator.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJa98ZQBIdQ9"
   },
   "source": [
    "The default configuration path for Accelerate is typically located in the user’s home directory under the .cache/huggingface/accelerate folder. The file is usually called default_config.yaml.\n",
    "\n",
    "Default Configuration Path:\n",
    "Path: ~/.cache/huggingface/accelerate/default_config.yaml\n",
    "You can also use the accelerate config command to configure or view settings interactively, which will save the configuration to this default file.\n",
    "This YAML file contains configuration settings for distributed training, including options like the number of processes, mixed precision, and device settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TpZ-Tmq2Hj3R"
   },
   "outputs": [],
   "source": [
    "# !accelerate config\n",
    "\n",
    "# \"\"\"\n",
    "# ~/.cache/huggingface/accelerate/default_config.yaml\n",
    "\n",
    "# compute_environment: LOCAL_MACHINE\n",
    "# debug: false\n",
    "# distributed_type: MULTI_GPU\n",
    "# downcast_bf16: 'no'\n",
    "# enable_cpu_affinity: false\n",
    "# gpu_ids: all\n",
    "# machine_rank: 0\n",
    "# main_training_function: main\n",
    "# mixed_precision: 'no'\n",
    "# num_machines: 1\n",
    "# num_processes: 2\n",
    "# rdzv_backend: static\n",
    "# same_network: true\n",
    "# tpu_env: []\n",
    "# tpu_use_cluster: false\n",
    "# tpu_use_sudo: false\n",
    "# use_cpu: false\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Accelerate vs. Torchrun for Distributed Training\n",
    "\n",
    "### Accelerate (using `accelerate launch` and `accelerate config` + `accelerate launch`)\n",
    "- **Flexibility & Features:**\n",
    "  - Offers a rich configuration interface to set up mixed precision, multi-node training, and automatic device placement.\n",
    "  - Provides many built-in utilities that simplify distributed training, such as automated logging and checkpoint management.\n",
    "- **Complexity:**\n",
    "  - The configuration step can be perceived as complex because it exposes many options and settings.\n",
    "  - This added complexity, however, translates into greater flexibility for a wide range of training scenarios.\n",
    "- **Ease of Use:**\n",
    "  - Once configured, running your script is straightforward with `accelerate launch` as it abstracts much of the distributed setup.\n",
    "\n",
    "### Torchrun\n",
    "- **Native Integration:**\n",
    "  - Torchrun is PyTorch’s native launcher for distributed training, requiring you to specify parameters like the number of processes per node.\n",
    "- **Flexibility:**\n",
    "  - It provides explicit control over distributed training without the additional abstraction layers.\n",
    "  - For users who prefer managing their distributed settings manually, it offers a more minimalistic interface.\n",
    "- **Complexity:**\n",
    "  - While torchrun is simpler in terms of the abstraction provided, it may require more manual setup for certain advanced features.\n",
    "  - Users familiar with PyTorch’s native distributed utilities often find it straightforward and less “heavy” than a full configuration tool.\n",
    "\n",
    "### Summary\n",
    "- **Accelerate**: More feature-rich and flexible, with extensive built-in automation that simplifies many aspects of distributed training. This flexibility comes with additional configuration complexity.\n",
    "- **Torchrun**: A more minimalistic and native solution that offers direct control over training parameters, which can be simpler for those comfortable with manual setup.\n",
    "\n",
    "Choose **Accelerate** if you need advanced features and automated configuration, and choose **Torchrun** if you prefer a lean, direct approach with explicit control.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMBuRkLzpKc48nA9BObSqYd",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
