{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jKjo3Lv-CJJe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6fcuxqECVbI"
   },
   "source": [
    "# Advanced Usage of Accelerate\n",
    "\n",
    "## 1. Mixed Precision Training\n",
    "\n",
    "### What is Mixed Precision Training?\n",
    "- Mixed precision training is a method to improve neural network training efficiency by combining 32-bit floating point (FP32) and 16-bit floating point (FP16/BF16).\n",
    "- This technique reduces memory usage, speeds up training, and maintains computational accuracy.\n",
    "\n",
    "### Mixed Precision Training Process:\n",
    "16bit weights â†’ 16bit loss â†’ 16bit gradients  \n",
    "   â†“ optimizer  \n",
    "32bit weights â†’ 32bit gradients\n",
    "\n",
    "Mixed precision training is a technique that accelerates deep learning model training and reduces memory usage by combining 16-bit (half-precision) and 32-bit (single-precision) floating-point computations. This approach leverages the performance benefits of lower-precision arithmetic while maintaining the accuracy and stability provided by higher-precision calculations.\n",
    "\n",
    "### Key Components of Mixed Precision Training\n",
    "\n",
    "1. **Half-Precision Computations (FP16)**  \n",
    "   Utilizing 16-bit floating-point numbers for operations such as matrix multiplications and convolutions reduces memory consumption and increases computational throughput. Modern GPUs, equipped with specialized hardware like NVIDIA's Tensor Cores, are optimized for FP16 operations, offering significant speedups.  \n",
    "   *[Source: NVIDIA Documentation](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html?utm_source=chatgpt.com)*  \n",
    "\n",
    "2. **Single-Precision Storage (FP32)**  \n",
    "   To preserve numerical accuracy, certain variables, especially model weights and bias parameters, are stored in 32-bit precision. This practice ensures that the reduced precision in computations doesn't lead to significant accuracy degradation.\n",
    "\n",
    "3. **Loss Scaling**  \n",
    "   During backpropagation, gradients can become very small and may underflow when represented in FP16. Loss scaling addresses this by multiplying the loss value by a predetermined factor before computing gradients, effectively scaling up the gradients. After the optimizer updates the weights, the gradients are scaled back down to maintain balance.  \n",
    "   *[Source: arXiv Paper](https://arxiv.org/abs/1710.03740?utm_source=chatgpt.com)*  \n",
    "\n",
    "### Process of Mixed Precision Training\n",
    "\n",
    "1. **Model Initialization**  \n",
    "   - Define the neural network model with weights initialized in 32-bit precision.\n",
    "\n",
    "2. **Casting to FP16**  \n",
    "   - Convert applicable parts of the model (e.g., layers, activations) to 16-bit precision for faster computation.\n",
    "\n",
    "3. **Forward Pass**  \n",
    "   - Perform computations using FP16 precision to take advantage of accelerated processing capabilities.\n",
    "\n",
    "4. **Loss Computation and Scaling**  \n",
    "   - Calculate the loss in FP16 and apply loss scaling to prevent gradient underflow.\n",
    "\n",
    "5. **Backward Pass**  \n",
    "   - Compute gradients in FP16 precision.\n",
    "\n",
    "6. **Unscaling Gradients**  \n",
    "   - Divide the scaled gradients by the loss scaling factor to return them to their original scale.\n",
    "\n",
    "7. **Gradient Casting to FP32**  \n",
    "   - Convert gradients back to 32-bit precision to ensure stable weight updates.\n",
    "\n",
    "8. **Weight Update**  \n",
    "   - Update the 32-bit precision weights using the optimizer.\n",
    "\n",
    "9. **Repeat**  \n",
    "   - Iterate through the forward and backward passes for each training step.\n",
    "\n",
    "By integrating mixed precision training, practitioners can achieve up to **3x speedups** in training times on compatible hardware, all while maintaining model accuracy.  \n",
    "*[Source: NVIDIA Documentation](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html?utm_source=chatgpt.com)*\n",
    "\n",
    "\n",
    "### How to Enable Mixed Precision Training:\n",
    "- **Method 1: Specify in Code**\n",
    "  - `accelerator = Accelerator(mixed_precision=\"bf16\")`\n",
    "- **Method 2: Use Configuration File**\n",
    "  - `accelerate config  # Select bf16`\n",
    "- **Method 3: Use Command Line**\n",
    "  - `accelerate launch --mixed_precision bf16 script.py`\n",
    "\n",
    "### Memory Usage Comparison: Mixed Precision vs. Single Precision\n",
    "| Component | Mixed Precision Training | Single Precision Training |\n",
    "|-----------|-------------------------|-------------------------|\n",
    "| Model | (4+2) Bytes * M | 4 Bytes * M |\n",
    "| Optimizer | 8 Bytes * M | 8 Bytes * M |\n",
    "| Gradients | (2+1) Bytes * A | 4 Bytes * A |\n",
    "| Buffer | 2 Bytes * A | 4 Bytes * A |\n",
    "| **Total** | (16+3) Bytes * M + 2 Bytes * A | 16 Bytes * M + 4 Bytes * A |\n",
    "\n",
    "> **Mixed precision training is useful for large batch sizes as it significantly reduces memory usage!**\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Gradient Accumulation\n",
    "\n",
    "### What is Gradient Accumulation?\n",
    "- Gradient accumulation allows training with larger effective batch sizes while using limited GPU memory.\n",
    "\n",
    "### Steps for Gradient Accumulation:\n",
    "- **Step 1: Create `Accelerator` and specify accumulation steps**\n",
    "  - `accelerator = Accelerator(gradient_accumulation_steps=xx)`\n",
    "- **Step 2: Apply `accelerator.accumulate(model)` during training**\n",
    "  - ```python\n",
    "    with accelerator.accumulate(model):\n",
    "        output = model(**batch)\n",
    "        loss = output.loss\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Logging (TensorBoard / WandB)\n",
    "\n",
    "### How to Enable Logging:\n",
    "- **Step 1: Create `Accelerator` and specify logging tool**\n",
    "  - `accelerator = Accelerator(log_with=\"tensorboard\", project_dir=\"logs/\")`\n",
    "  - Or use WandB:\n",
    "  - `accelerator = Accelerator(log_with=\"wandb\", project_dir=\"wandb_logs/\")`\n",
    "\n",
    "- **Step 2: Initialize Logger**\n",
    "  - `accelerator.init_trackers(project_name=\"my_experiment\")`\n",
    "\n",
    "- **Step 3: End Training and Save Logs**\n",
    "  - `accelerator.end_training()`\n",
    "\n",
    "> With TensorBoard or WandB, you can monitor the training process in real-time!\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Model Saving & Resuming\n",
    "\n",
    "### 4.1 Saving the Model\n",
    "- **Method 1: Directly Save the Model**\n",
    "  - `accelerator.save_model(model, \"model_checkpoint/\")`\n",
    "  - Saves only model parameters, not optimizer states.\n",
    "  - Fully saves PEFT (parameter-efficient fine-tuning) models.\n",
    "\n",
    "### 4.2 Checkpointing (Saving & Resuming Training)\n",
    "#### **How to Save Training State**\n",
    "  - `accelerator.save_state(\"checkpoint_dir/\")`\n",
    "\n",
    "#### **How to Load Training State**\n",
    "  - `accelerator.load_state(\"checkpoint_dir/\")`\n",
    "\n",
    "#### **Compute Resumption Steps**\n",
    "  - `resume_epoch, resume_step = accelerator.load_state()`\n",
    "\n",
    "#### **Skip Already Processed Batches**\n",
    "  - `accelerator.skip_first_batches(trainloader, resume_step)`\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "Accelerate provides an all-in-one solution for optimizing deep learning training:\n",
    "- âœ… **Mixed Precision Training** â†’ Increases computation speed, reduces memory usage.\n",
    "- âœ… **Gradient Accumulation** â†’ Simulates large batch training with small GPU memory.\n",
    "- âœ… **Logging** â†’ Easily monitor training with TensorBoard / WandB.\n",
    "- âœ… **Model Saving & Resumption** â†’ Resume interrupted training seamlessly.\n",
    "- âœ… **Distributed Training** â†’ Simplifies multi-GPU / TPU training.\n",
    "\n",
    "ðŸš€ Let `accelerate` help you train deep learning models efficiently!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qSFt1YjTCVzT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ddp_accelerator2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ddp_accelerator2.py\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import Adam\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Initialize the Accelerator\n",
    "accelerator = Accelerator(\n",
    "    mixed_precision=\"bf16\",\n",
    "    gradient_accumulation_steps=2, \n",
    "    log_with=\"tensorboard\", project_dir=\"logs\")\n",
    "accelerator.init_trackers(\"runs\")\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"google-bert/bert-base-cased\", num_labels=5, torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "# Define a custom Dataset class\n",
    "class YelpReviewDataset(Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.dataset = dataset[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        text = item['text']\n",
    "        label = item['label']\n",
    "        return text, label\n",
    "\n",
    "# Instantiate the datasets\n",
    "train_dataset = YelpReviewDataset(split='train')\n",
    "test_dataset = YelpReviewDataset(split='test')\n",
    "\n",
    "# Function to create a random subset of the dataset\n",
    "def create_subset_indices(dataset, num_samples):\n",
    "    indices = list(range(len(dataset)))\n",
    "    random.seed(42)  # For reproducibility\n",
    "    random.shuffle(indices)\n",
    "    return indices[:num_samples]\n",
    "\n",
    "# Create subsets\n",
    "train_indices = create_subset_indices(train_dataset, 1000)\n",
    "test_indices = create_subset_indices(test_dataset, 500)\n",
    "\n",
    "train_subset = Subset(train_dataset, train_indices)\n",
    "test_subset = Subset(test_dataset, test_indices)\n",
    "\n",
    "# Define the collate function for tokenization\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    inputs = tokenizer(\n",
    "        list(texts),\n",
    "        max_length=128,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs[\"labels\"] = torch.tensor(labels)\n",
    "    return inputs\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_subset,\n",
    "    batch_size=32,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    test_subset,\n",
    "    batch_size=64,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Optimizer setup\n",
    "optimizer = Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "# # Function to ensure only rank 0 prints log messages\n",
    "# # Can be replace by accelerator.print\n",
    "# def print_rank_0(info):\n",
    "#     if accelerator.is_local_main_process:\n",
    "#         print(info)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(ddp_model, ddp_valid_loader):\n",
    "    ddp_model.eval()\n",
    "    acc_num = 0\n",
    "    with torch.no_grad():   # torch.inference_mode() will raise error for deepspeed zero3 training. See in the deepspeed notebook\n",
    "        for batch in ddp_valid_loader:\n",
    "            output = ddp_model(**batch)\n",
    "            pred = torch.argmax(output.logits, dim=-1)\n",
    "            # Gather predictions and references\n",
    "            pred, refs = accelerator.gather_for_metrics((pred, batch[\"labels\"]))\n",
    "            # Ensure predictions and references are on the same device for comparison\n",
    "            acc_num += (pred.long() == refs.long()).float().sum()\n",
    "\n",
    "    return acc_num / len(ddp_valid_loader.dataset)\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train(epochs=3, log_step=10, resume=None):\n",
    "    global_step = 0\n",
    "    ddp_model, ddp_optimizer, ddp_train_loader, ddp_valid_loader = accelerator.prepare(\n",
    "        model, optimizer, train_loader, valid_loader\n",
    "    )\n",
    "\n",
    "    resume_step = 0\n",
    "    resume_epoch = 0\n",
    "\n",
    "    if resume is not None:\n",
    "        accelerator.load_state(resume)\n",
    "        steps_per_epoch = math.ceil(len(trainloader) / accelerator.gradient_accumulation_steps)\n",
    "        resume_step = global_step = int(resume.split(\"step_\")[-1])\n",
    "        resume_epoch = resume_step // steps_per_epoch\n",
    "        resume_step -= resume_epoch * steps_per_epoch\n",
    "        accelerator.print(f\"resume from checkpoint -> {resume}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        ddp_model.train()\n",
    "        if resume and ep == resume_epoch and resume_step != 0:\n",
    "            active_dataloader = accelerator.skip_first_batches(ddp_train_loader, resume_step * accelerator.gradient_accumulation_steps)\n",
    "        else:\n",
    "            active_dataloader = ddp_train_loader\n",
    "        for batch in active_dataloader:\n",
    "            with accelerator.accumulate(ddp_model):\n",
    "                ddp_optimizer.zero_grad()\n",
    "                output = ddp_model(**batch)\n",
    "                loss = output.loss\n",
    "                accelerator.backward(loss)  # accelerator backward\n",
    "                ddp_optimizer.step()\n",
    "\n",
    "                if accelerator.sync_gradients:\n",
    "                    global_step += 1\n",
    "\n",
    "                    if global_step % log_step == 0:\n",
    "                        loss = accelerator.reduce(loss, \"mean\")\n",
    "                        accelerator.print(f\"ep: {ep}, global_step: {global_step}, loss: {loss.item()}\")\n",
    "                        accelerator.log({\"loss\": loss.item()}, global_step)\n",
    "\n",
    "                    if global_step % 10 == 0 and global_step != 0:\n",
    "                        accelerator.print(f\"save checkpoint -> step_{global_step}\")\n",
    "                        accelerator.save_state(accelerator.project_dir + f\"/step_{global_step}\")\n",
    "                        accelerator.unwrap_model(ddp_model).save_pretrained(\n",
    "                            save_directory=accelerator.project_dir + f\"/step_{global_step}/model\",\n",
    "                            is_main_process=accelerator.is_main_process,\n",
    "                            state_dict=accelerator.get_state_dict(ddp_model),\n",
    "                            save_func=accelerator.save\n",
    "                        )\n",
    "\n",
    "        # Evaluate the model after each epoch\n",
    "        acc = evaluate(ddp_model, ddp_valid_loader)\n",
    "        accelerator.print(f\"Epoch: {epoch}, Accuracy: {acc}\")\n",
    "\n",
    "# Start training\n",
    "train()\n",
    "\n",
    "# Cleanup\n",
    "accelerator.end_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ExO8hoSCET2K"
   },
   "outputs": [],
   "source": [
    "# !torchrun --nproc_per_node=2 ddp_accelerator2.py\n",
    "# !accelerate launch ddp_accelerator2.py\n",
    "# !accelerate config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJa98ZQBIdQ9"
   },
   "source": [
    "The default configuration path for Accelerate is typically located in the userâ€™s home directory under the .cache/huggingface/accelerate folder. The file is usually called default_config.yaml.\n",
    "\n",
    "Default Configuration Path:\n",
    "Path: ~/.cache/huggingface/accelerate/default_config.yaml\n",
    "You can also use the accelerate config command to configure or view settings interactively, which will save the configuration to this default file.\n",
    "This YAML file contains configuration settings for distributed training, including options like the number of processes, mixed precision, and device settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TpZ-Tmq2Hj3R"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMLYrfXD51Y7MUBW7kY1NZ7",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
