{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jKjo3Lv-CJJe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6fcuxqECVbI"
   },
   "source": [
    "# DeepSpeed with Accelerate\n",
    "\n",
    "## DeepSpeed Overview\n",
    "DeepSpeed is an optimization framework developed by Microsoft to simplify and enhance distributed deep learning training. The key components include:\n",
    "\n",
    "### ZeRO (Zero Redundancy Optimizer)\n",
    "DeepSpeed uses **ZeRO** to reduce memory usage and improve training efficiency. There are three levels of ZeRO:\n",
    "\n",
    "1. **ZeRO-1 (Optimizer States Optimization)**\n",
    "   - Reduces memory by partitioning optimizer states across GPUs.\n",
    "   \n",
    "2. **ZeRO-2 (Optimizer States + Gradient Optimization)**\n",
    "   - Further reduces memory by partitioning gradients along with optimizer states.\n",
    "   - Communication volume remains the same compared to **DDP (Distributed Data Parallel)**.\n",
    "\n",
    "3. **ZeRO-3 (Optimizer States + Gradients + Parameters Optimization)**\n",
    "   - Fully shards model states across GPUs, reducing memory overhead.\n",
    "   - Requires additional communication, increasing overall communication volume by **1.5x** compared to DDP.\n",
    "\n",
    "### Advanced ZeRO Features\n",
    "- **ZeRO Offload**: Moves optimizer states and model parameters to CPU or NVMe memory for further memory savings.\n",
    "- **ZeRO++**: Improves ZeRO-3 communication efficiency using weight quantization and hierarchical storage.\n",
    "- **DeepSpeed Ulysses**: Optimized for sequential training, distributing each sample across GPUs.\n",
    "\n",
    "## DeepSpeed with Accelerate Integration\n",
    "Accelerate provides a seamless way to configure and run DeepSpeed in distributed environments.\n",
    "\n",
    "### Method 1: Basic Integration\n",
    "1. Configure DeepSpeed settings using the `accelerate config` command.\n",
    "2. Launch the script with `accelerate launch`.\n",
    "\n",
    "### Method 2: Full DeepSpeed Configuration\n",
    "1. Configure DeepSpeed settings using `accelerate config`.\n",
    "2. Modify the **DeepSpeed config file** (`config.json`) as needed.\n",
    "3. Launch training with `accelerate launch`.\n",
    "\n",
    "### ZeRO-3 Considerations\n",
    "- `zero3_init_flag`: Used to load models efficiently.\n",
    "- `zero3_save_16bit_model`: Saves models in **16-bit precision** to reduce storage.\n",
    "- If `config.json` specifies `stage3_gather_16bit_weights_on_model_save`, it ensures weights are gathered correctly before saving.\n",
    "- Evaluation should be performed within a no-gradient context.\n",
    "\n",
    "### Multi-Machine & Multi-GPU Training\n",
    "- In **direct launch mode**, users must specify settings like `deepspeed_hostfile`, `num_machines`, and `main_process_port`.\n",
    "- Under **SLURM management**, the launcher is configured similarly to `torchrun`, specifying parameters like `num_machines`, `machine_rank`, and `main_process_ip`.\n",
    "- Accelerate launch behaves similarly to `torchrun` in this case.\n",
    "\n",
    "## Key Takeaways\n",
    "- **DeepSpeed** optimizes large-scale training using **ZeRO** techniques.\n",
    "- **Accelerate** simplifies DeepSpeed configuration and deployment.\n",
    "- **ZeRO-3 + Offloading** can significantly reduce GPU memory usage.\n",
    "- **Accelerate works with both local and multi-machine training.**\n",
    "\n",
    "This integration allows efficient training of large-scale models on multiple GPUs with minimal manual setup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSFt1YjTCVzT"
   },
   "source": [
    "## Launch zero2 with Method 1\n",
    "Here we can use the same __ddp_accelerator.py__ script created from previous notebooks to run. Use ddp_accelerator2.py will have errors for now as deepspeed doesn't work with accelerate.accumulate(). But you can try and build on it on your own.  \n",
    "Here I copied my config files to `deepspeed_config.yaml` to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ExO8hoSCET2K"
   },
   "outputs": [],
   "source": [
    "# !accelerate config\n",
    "# !accelerate launch --config_file deepspeed_config.yaml ddp_accelerator.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpZ-Tmq2Hj3R"
   },
   "source": [
    "## Launch zero2 with Method 2\n",
    "In this method, you can prepare the configs file (which you can do more customizations) to run. Configs and details see this [document](https://huggingface.co/docs/accelerate/en/usage_guides/deepspeed)  \n",
    "Here we use config files to `deepspeed_config2.yaml` and `zero_stage2_config.json` to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !accelerate launch --config_file deepspeed_config2.yaml ddp_accelerator.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch zero3 with both Methods\n",
    "For zero3 there are also some configs need to be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !accelerate launch --config_file deepspeed_config3.yaml ddp_accelerator.py\n",
    "# !accelerate launch --config_file deepspeed_config4.yaml ddp_accelerator.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch zero2 with Trainer\n",
    "We can also run the previous __ddp_trainer.py__ using deepspeed configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !accelerate launch --config_file deepspeed_config.yaml ddp_trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMLYrfXD51Y7MUBW7kY1NZ7",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
