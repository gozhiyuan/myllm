# ðŸš€ LLM & AI Code Recipes ðŸš€

This repository serves as a collection of practical tutorials, code examples, and reference implementations for various concepts in Large Language Models (LLMs) and AI. The projects within are designed to be clear, well-documented, and reusable for educational and practical purposes.

Each folder contains a standalone project or tutorial.

---

## ðŸ“‚ Projects & Tutorials

Here are the projects currently available. This collection is actively maintained and expanded.

### [distributed_training_accelerate](./distributed_training_accelerate/)

A comprehensive guide to distributed model training in PyTorch. This project provides a series of hands-on examples that progress from basic single-GPU training to advanced, large-scale distributed setups using industry-standard tools.

**Core Concepts Covered:**
*   **Data Parallelism (DP)** vs. **Distributed Data Parallelism (DDP)**
*   Simplifying distributed training with **Hugging Face `accelerate`**
*   Optimizing large model training with **DeepSpeed**, including ZeRO stages.

The project includes a sequence of Jupyter notebooks and Python scripts to illustrate these concepts clearly.

### [mini_lm](./mini_lm/)

A project focused on building and training a "smolLM" (a small-scale Language Model) from scratch. This project will cover the entire lifecycle of creating an LLM, providing a deep dive into the underlying mechanics.

**Key Learning Objectives:**
*   Implementing the core components of a Transformer-based language model.
*   Building a data loading and preprocessing pipeline.
*   Training the model from the ground up.
*   Implementing techniques for efficient training and inference.


---

Feel free to explore the projects and adapt the code for your own use cases.
